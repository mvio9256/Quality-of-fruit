{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1,2,3,4,5'\n",
    "start_time = time.time()\n",
    "batch_size = 64\n",
    "learning_rate = 0.003\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = datasets.ImageFolder(\"./data/train\",\n",
    "                         transform=transforms.Compose([transforms.Resize(128),\n",
    "                                                       transforms.RandomCrop(64),\n",
    "                                                       transforms.ToTensor()]))\n",
    "\n",
    "test_imgs = datasets.ImageFolder(\"./data/test\",\n",
    "                        transform=transforms.Compose([transforms.Resize(128),\n",
    "                                                      transforms.RandomCrop(64),\n",
    "                                                      transforms.ToTensor()]))\n",
    "\n",
    "train_loader = DataLoader(train_imgs, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_imgs, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 5 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
      "              ReLU-2           [-1, 64, 64, 64]               0\n",
      "            Conv2d-3           [-1, 64, 64, 64]          36,928\n",
      "              ReLU-4           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-5           [-1, 64, 32, 32]               0\n",
      "            Conv2d-6          [-1, 128, 32, 32]          73,856\n",
      "              ReLU-7          [-1, 128, 32, 32]               0\n",
      "            Conv2d-8          [-1, 128, 32, 32]         147,584\n",
      "              ReLU-9          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-10          [-1, 128, 16, 16]               0\n",
      "           Conv2d-11          [-1, 256, 16, 16]         295,168\n",
      "             ReLU-12          [-1, 256, 16, 16]               0\n",
      "           Conv2d-13          [-1, 256, 16, 16]         590,080\n",
      "             ReLU-14          [-1, 256, 16, 16]               0\n",
      "           Conv2d-15          [-1, 256, 16, 16]         590,080\n",
      "             ReLU-16          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-17            [-1, 256, 8, 8]               0\n",
      "           Conv2d-18            [-1, 512, 8, 8]       1,180,160\n",
      "             ReLU-19            [-1, 512, 8, 8]               0\n",
      "           Conv2d-20            [-1, 512, 8, 8]       2,359,808\n",
      "             ReLU-21            [-1, 512, 8, 8]               0\n",
      "           Conv2d-22            [-1, 512, 8, 8]       2,359,808\n",
      "             ReLU-23            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-24            [-1, 512, 4, 4]               0\n",
      "           Conv2d-25            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-26            [-1, 512, 4, 4]               0\n",
      "           Conv2d-27            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-28            [-1, 512, 4, 4]               0\n",
      "           Conv2d-29            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-30            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-31            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "              VGG-40                 [-1, 1000]               0\n",
      "           Conv2d-41           [-1, 64, 64, 64]           1,792\n",
      "             ReLU-42           [-1, 64, 64, 64]               0\n",
      "           Conv2d-43           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-44           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-45           [-1, 64, 32, 32]               0\n",
      "           Conv2d-46          [-1, 128, 32, 32]          73,856\n",
      "             ReLU-47          [-1, 128, 32, 32]               0\n",
      "           Conv2d-48          [-1, 128, 32, 32]         147,584\n",
      "             ReLU-49          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-50          [-1, 128, 16, 16]               0\n",
      "           Conv2d-51          [-1, 256, 16, 16]         295,168\n",
      "             ReLU-52          [-1, 256, 16, 16]               0\n",
      "           Conv2d-53          [-1, 256, 16, 16]         590,080\n",
      "             ReLU-54          [-1, 256, 16, 16]               0\n",
      "           Conv2d-55          [-1, 256, 16, 16]         590,080\n",
      "             ReLU-56          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-57            [-1, 256, 8, 8]               0\n",
      "           Conv2d-58            [-1, 512, 8, 8]       1,180,160\n",
      "             ReLU-59            [-1, 512, 8, 8]               0\n",
      "           Conv2d-60            [-1, 512, 8, 8]       2,359,808\n",
      "             ReLU-61            [-1, 512, 8, 8]               0\n",
      "           Conv2d-62            [-1, 512, 8, 8]       2,359,808\n",
      "             ReLU-63            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-64            [-1, 512, 4, 4]               0\n",
      "           Conv2d-65            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-66            [-1, 512, 4, 4]               0\n",
      "           Conv2d-67            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-68            [-1, 512, 4, 4]               0\n",
      "           Conv2d-69            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-70            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-71            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-72            [-1, 512, 7, 7]               0\n",
      "           Linear-73                 [-1, 4096]     102,764,544\n",
      "             ReLU-74                 [-1, 4096]               0\n",
      "          Dropout-75                 [-1, 4096]               0\n",
      "           Linear-76                 [-1, 4096]      16,781,312\n",
      "             ReLU-77                 [-1, 4096]               0\n",
      "          Dropout-78                 [-1, 4096]               0\n",
      "           Linear-79                 [-1, 1000]       4,097,000\n",
      "              VGG-80                 [-1, 1000]               0\n",
      "================================================================\n",
      "Total params: 276,715,088\n",
      "Trainable params: 276,715,088\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 36.44\n",
      "Params size (MB): 1055.58\n",
      "Estimated Total Size (MB): 1092.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(vgg16).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(vgg16.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (6.9111) | Acc_1: (0.00%) (0/64)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (6.8679) | Acc_1: (12.22%) (86/704)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (6.2132) | Acc_1: (18.30%) (246/1344)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (6.0392) | Acc_1: (19.27%) (374/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (3.3431) | Acc: (30.10%) (31/103)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (3.1338) | Acc_1: (23.44%) (15/64)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (2.3618) | Acc_1: (24.01%) (169/704)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (1.9313) | Acc_1: (24.78%) (333/1344)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (1.7570) | Acc_1: (26.22%) (509/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.4166) | Acc: (19.42%) (20/103)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (1.4060) | Acc_1: (26.56%) (17/64)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.4136) | Acc_1: (26.85%) (189/704)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.4090) | Acc_1: (28.57%) (384/1344)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.4020) | Acc_1: (28.65%) (556/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.4589) | Acc: (30.10%) (31/103)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.4789) | Acc_1: (29.69%) (19/64)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (1.4068) | Acc_1: (28.41%) (200/704)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (1.3929) | Acc_1: (29.24%) (393/1344)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (1.3714) | Acc_1: (29.98%) (582/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.2714) | Acc: (46.60%) (48/103)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (1.3131) | Acc_1: (39.06%) (25/64)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (1.3004) | Acc_1: (39.06%) (275/704)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (1.3057) | Acc_1: (39.43%) (530/1344)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (1.2781) | Acc_1: (41.83%) (812/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0782) | Acc: (46.60%) (48/103)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (1.0615) | Acc_1: (57.81%) (37/64)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (1.1087) | Acc_1: (46.45%) (327/704)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (1.1174) | Acc_1: (48.66%) (654/1344)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (1.0974) | Acc_1: (49.20%) (955/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0563) | Acc: (49.51%) (51/103)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (1.0594) | Acc_1: (54.69%) (35/64)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (1.1282) | Acc_1: (49.01%) (345/704)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (1.0866) | Acc_1: (49.48%) (665/1344)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (1.0560) | Acc_1: (50.23%) (975/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0190) | Acc: (54.37%) (56/103)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (1.1097) | Acc_1: (43.75%) (28/64)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (1.0462) | Acc_1: (49.29%) (347/704)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (1.0632) | Acc_1: (48.81%) (656/1344)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (1.0552) | Acc_1: (50.33%) (977/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0621) | Acc: (48.54%) (50/103)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (1.0901) | Acc_1: (48.44%) (31/64)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (1.0253) | Acc_1: (50.57%) (356/704)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (1.0011) | Acc_1: (50.22%) (675/1344)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (0.9824) | Acc_1: (52.55%) (1020/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.3623) | Acc: (33.01%) (34/103)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (2.3187) | Acc_1: (32.81%) (21/64)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (1.3069) | Acc_1: (41.62%) (293/704)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (1.1713) | Acc_1: (46.88%) (630/1344)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (1.1355) | Acc_1: (47.86%) (929/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9591) | Acc: (48.54%) (50/103)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (1.0345) | Acc_1: (50.00%) (32/64)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (0.9571) | Acc_1: (56.39%) (397/704)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (0.9177) | Acc_1: (56.62%) (761/1344)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (0.9518) | Acc_1: (54.87%) (1065/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0802) | Acc: (50.49%) (52/103)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (1.1070) | Acc_1: (50.00%) (32/64)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.9675) | Acc_1: (52.13%) (367/704)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (0.9449) | Acc_1: (54.91%) (738/1344)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (0.9502) | Acc_1: (55.13%) (1070/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8896) | Acc: (58.25%) (60/103)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (0.8785) | Acc_1: (59.38%) (38/64)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.9694) | Acc_1: (54.83%) (386/704)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.9507) | Acc_1: (55.06%) (740/1344)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.9321) | Acc_1: (56.31%) (1093/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1968) | Acc: (50.49%) (52/103)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (1.2002) | Acc_1: (48.44%) (31/64)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.9887) | Acc_1: (54.83%) (386/704)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.9553) | Acc_1: (55.73%) (749/1344)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.9121) | Acc_1: (57.24%) (1111/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9301) | Acc: (47.57%) (49/103)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.8603) | Acc_1: (62.50%) (40/64)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.9310) | Acc_1: (56.25%) (396/704)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.9154) | Acc_1: (57.44%) (772/1344)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.8802) | Acc_1: (58.58%) (1137/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0084) | Acc: (55.34%) (57/103)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.9566) | Acc_1: (54.69%) (35/64)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.9007) | Acc_1: (56.82%) (400/704)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.9266) | Acc_1: (54.84%) (737/1344)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.8826) | Acc_1: (56.98%) (1106/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8586) | Acc: (58.25%) (60/103)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.7470) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.8886) | Acc_1: (57.10%) (402/704)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.8505) | Acc_1: (60.64%) (815/1344)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.8323) | Acc_1: (60.12%) (1167/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1991) | Acc: (53.40%) (55/103)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (1.0922) | Acc_1: (51.56%) (33/64)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.7950) | Acc_1: (61.79%) (435/704)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.8282) | Acc_1: (60.42%) (812/1344)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.8292) | Acc_1: (60.33%) (1171/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8233) | Acc: (56.31%) (58/103)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.7365) | Acc_1: (60.94%) (39/64)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.8037) | Acc_1: (58.10%) (409/704)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.7960) | Acc_1: (60.64%) (815/1344)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.8120) | Acc_1: (61.10%) (1186/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8856) | Acc: (55.34%) (57/103)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.7167) | Acc_1: (60.94%) (39/64)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.8061) | Acc_1: (62.93%) (443/704)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.7992) | Acc_1: (61.31%) (824/1344)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.8001) | Acc_1: (61.15%) (1187/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8261) | Acc: (61.17%) (63/103)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.8064) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.7408) | Acc_1: (63.07%) (444/704)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.7722) | Acc_1: (64.14%) (862/1344)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.7796) | Acc_1: (63.27%) (1228/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7504) | Acc: (62.14%) (64/103)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.6760) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.7462) | Acc_1: (60.51%) (426/704)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.7678) | Acc_1: (62.65%) (842/1344)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.7513) | Acc_1: (64.04%) (1243/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0463) | Acc: (46.60%) (48/103)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (1.0984) | Acc_1: (51.56%) (33/64)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.8273) | Acc_1: (61.51%) (433/704)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.9200) | Acc_1: (60.64%) (815/1344)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.8787) | Acc_1: (61.15%) (1187/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9053) | Acc: (54.37%) (56/103)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.7837) | Acc_1: (56.25%) (36/64)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.8038) | Acc_1: (63.92%) (450/704)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.7730) | Acc_1: (63.84%) (858/1344)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.7716) | Acc_1: (63.21%) (1227/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8177) | Acc: (58.25%) (60/103)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.7666) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.6999) | Acc_1: (65.91%) (464/704)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.6993) | Acc_1: (66.96%) (900/1344)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.7112) | Acc_1: (66.10%) (1283/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7451) | Acc: (62.14%) (64/103)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.5835) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.7162) | Acc_1: (64.91%) (457/704)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.6844) | Acc_1: (65.40%) (879/1344)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.6989) | Acc_1: (65.07%) (1263/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7541) | Acc: (60.19%) (62/103)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.7078) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.7293) | Acc_1: (65.48%) (461/704)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.8211) | Acc_1: (60.94%) (819/1344)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.8280) | Acc_1: (60.33%) (1171/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7724) | Acc: (60.19%) (62/103)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.7456) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.8878) | Acc_1: (61.08%) (430/704)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.8317) | Acc_1: (60.57%) (814/1344)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.8293) | Acc_1: (60.84%) (1181/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8042) | Acc: (64.08%) (66/103)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.7090) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.7757) | Acc_1: (62.78%) (442/704)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.7462) | Acc_1: (64.58%) (868/1344)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.7382) | Acc_1: (64.66%) (1255/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8796) | Acc: (56.31%) (58/103)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.8340) | Acc_1: (59.38%) (38/64)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.7869) | Acc_1: (65.48%) (461/704)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.7241) | Acc_1: (66.74%) (897/1344)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.7267) | Acc_1: (65.48%) (1271/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7545) | Acc: (64.08%) (66/103)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.7733) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.7082) | Acc_1: (66.90%) (471/704)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.6874) | Acc_1: (66.74%) (897/1344)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.6716) | Acc_1: (68.06%) (1321/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9290) | Acc: (58.25%) (60/103)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.6701) | Acc_1: (60.94%) (39/64)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.6901) | Acc_1: (66.76%) (470/704)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.6754) | Acc_1: (68.30%) (918/1344)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.6635) | Acc_1: (69.19%) (1343/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8384) | Acc: (56.31%) (58/103)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.5335) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.7064) | Acc_1: (67.05%) (472/704)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.7172) | Acc_1: (66.67%) (896/1344)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.6991) | Acc_1: (67.70%) (1314/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6992) | Acc: (65.05%) (67/103)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.6982) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.6404) | Acc_1: (69.74%) (491/704)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.6419) | Acc_1: (69.49%) (934/1344)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.6453) | Acc_1: (69.91%) (1357/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8976) | Acc: (59.22%) (61/103)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.8645) | Acc_1: (54.69%) (35/64)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.7360) | Acc_1: (64.77%) (456/704)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.6990) | Acc_1: (67.26%) (904/1344)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.6672) | Acc_1: (68.06%) (1321/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8191) | Acc: (62.14%) (64/103)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.5852) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.6326) | Acc_1: (69.60%) (490/704)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.6384) | Acc_1: (69.87%) (939/1344)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.6509) | Acc_1: (69.04%) (1340/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9128) | Acc: (58.25%) (60/103)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.7611) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.6935) | Acc_1: (69.46%) (489/704)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.6539) | Acc_1: (70.31%) (945/1344)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.6417) | Acc_1: (70.43%) (1367/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7724) | Acc: (65.05%) (67/103)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.6300) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.6132) | Acc_1: (70.60%) (497/704)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.6350) | Acc_1: (70.46%) (947/1344)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.6266) | Acc_1: (70.89%) (1376/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7381) | Acc: (61.17%) (63/103)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.5985) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.7138) | Acc_1: (69.46%) (489/704)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.6752) | Acc_1: (69.79%) (938/1344)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.6565) | Acc_1: (70.89%) (1376/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7717) | Acc: (67.96%) (70/103)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.6786) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.6538) | Acc_1: (69.03%) (486/704)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.6180) | Acc_1: (70.61%) (949/1344)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.6062) | Acc_1: (71.25%) (1383/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7167) | Acc: (65.05%) (67/103)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.6146) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.5940) | Acc_1: (74.43%) (524/704)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.6075) | Acc_1: (73.29%) (985/1344)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.5870) | Acc_1: (73.67%) (1430/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7080) | Acc: (66.02%) (68/103)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.5850) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.5698) | Acc_1: (74.43%) (524/704)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.5620) | Acc_1: (74.78%) (1005/1344)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.6299) | Acc_1: (72.49%) (1407/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6620) | Acc: (68.93%) (71/103)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.7107) | Acc_1: (79.69%) (51/64)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.5745) | Acc_1: (75.85%) (534/704)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.5739) | Acc_1: (75.60%) (1016/1344)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.5553) | Acc_1: (75.73%) (1470/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5817) | Acc: (75.73%) (78/103)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.7872) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.5558) | Acc_1: (77.41%) (545/704)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.6116) | Acc_1: (74.70%) (1004/1344)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.6054) | Acc_1: (74.03%) (1437/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8396) | Acc: (62.14%) (64/103)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.7314) | Acc_1: (71.88%) (46/64)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.5594) | Acc_1: (75.85%) (534/704)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.5707) | Acc_1: (75.15%) (1010/1344)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.5437) | Acc_1: (76.76%) (1490/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1497) | Acc: (61.17%) (63/103)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.9669) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.5599) | Acc_1: (76.70%) (540/704)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.5398) | Acc_1: (77.16%) (1037/1344)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.5101) | Acc_1: (78.46%) (1523/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7578) | Acc: (69.90%) (72/103)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.7961) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.5215) | Acc_1: (79.55%) (560/704)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.4888) | Acc_1: (81.25%) (1092/1344)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.5253) | Acc_1: (78.57%) (1525/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6949) | Acc: (63.11%) (65/103)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.7084) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.5364) | Acc_1: (79.83%) (562/704)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.5761) | Acc_1: (75.37%) (1013/1344)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.5555) | Acc_1: (76.46%) (1484/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3483) | Acc: (48.54%) (50/103)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (1.1403) | Acc_1: (56.25%) (36/64)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.5593) | Acc_1: (77.27%) (544/704)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.5211) | Acc_1: (79.02%) (1062/1344)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.5119) | Acc_1: (79.34%) (1540/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9410) | Acc: (70.87%) (73/103)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.6631) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.5909) | Acc_1: (76.70%) (540/704)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.5413) | Acc_1: (78.79%) (1059/1344)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.5090) | Acc_1: (80.16%) (1556/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5366) | Acc: (78.64%) (81/103)\n",
      "2 hours 29 mins 3 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='vgg16.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 50):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='vgg16.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = load_checkpoint(default_directory, filename='vgg16.tar.gz')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_img(img_path, model):\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # opencv는 BGR순서로 read한다.\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    img = torch.from_numpy(img).float()\n",
    "\n",
    "    img = img.permute(2, 0, 1).squeeze(0) # (H, W, C) -> (C, H, W) -> (1, C, H, W)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.Softmax()\n",
    "\n",
    "    out = model(img)\n",
    "    label_idx = torch.argmax(out, dim=1)\n",
    "    prob = criterion(out)\n",
    "\n",
    "    return prob, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/hom1/ict01/.conda/envs/tomato_cls/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 hours 0 mins 8 secs for training\n"
     ]
    }
   ],
   "source": [
    "default_directory = './save_models'\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='vgg16.tar.gz')\n",
    "\n",
    "# model편에서 만들어 두었던 MyNetwork를 활용.\n",
    "\n",
    "model = models.vgg16()\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "#model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for img in glob.iglob('./data/test/**/*.jpg', recursive=True):\n",
    "    pred, label_idx = classification_img(img, model)\n",
    "\n",
    "now = time.gmtime(time.time() - start)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea228a1e57bab7175f6663bb7b03ad0c48b776b0d8531c957bf1be729535464c"
  },
  "kernelspec": {
   "display_name": "tomato_cls",
   "language": "python",
   "name": "tomato_cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
