{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1,2,3,4,5'\n",
    "start_time = time.time()\n",
    "batch_size = 64\n",
    "learning_rate = 0.003\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/vgg19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = datasets.ImageFolder(\"./data/train\",\n",
    "                         transform=transforms.Compose([transforms.Resize(64),\n",
    "                                                       transforms.RandomCrop(45),\n",
    "                                                       transforms.ToTensor()]))\n",
    "\n",
    "test_imgs = datasets.ImageFolder(\"./data/test\",\n",
    "                        transform=transforms.Compose([transforms.Resize(64),\n",
    "                                                      transforms.RandomCrop(45),\n",
    "                                                      transforms.ToTensor()]))\n",
    "\n",
    "train_loader = DataLoader(train_imgs, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_imgs, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = models.vgg19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 5 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 45, 45]           1,792\n",
      "              ReLU-2           [-1, 64, 45, 45]               0\n",
      "            Conv2d-3           [-1, 64, 45, 45]          36,928\n",
      "              ReLU-4           [-1, 64, 45, 45]               0\n",
      "         MaxPool2d-5           [-1, 64, 22, 22]               0\n",
      "            Conv2d-6          [-1, 128, 22, 22]          73,856\n",
      "              ReLU-7          [-1, 128, 22, 22]               0\n",
      "            Conv2d-8          [-1, 128, 22, 22]         147,584\n",
      "              ReLU-9          [-1, 128, 22, 22]               0\n",
      "        MaxPool2d-10          [-1, 128, 11, 11]               0\n",
      "           Conv2d-11          [-1, 256, 11, 11]         295,168\n",
      "             ReLU-12          [-1, 256, 11, 11]               0\n",
      "           Conv2d-13          [-1, 256, 11, 11]         590,080\n",
      "             ReLU-14          [-1, 256, 11, 11]               0\n",
      "           Conv2d-15          [-1, 256, 11, 11]         590,080\n",
      "             ReLU-16          [-1, 256, 11, 11]               0\n",
      "           Conv2d-17          [-1, 256, 11, 11]         590,080\n",
      "             ReLU-18          [-1, 256, 11, 11]               0\n",
      "        MaxPool2d-19            [-1, 256, 5, 5]               0\n",
      "           Conv2d-20            [-1, 512, 5, 5]       1,180,160\n",
      "             ReLU-21            [-1, 512, 5, 5]               0\n",
      "           Conv2d-22            [-1, 512, 5, 5]       2,359,808\n",
      "             ReLU-23            [-1, 512, 5, 5]               0\n",
      "           Conv2d-24            [-1, 512, 5, 5]       2,359,808\n",
      "             ReLU-25            [-1, 512, 5, 5]               0\n",
      "           Conv2d-26            [-1, 512, 5, 5]       2,359,808\n",
      "             ReLU-27            [-1, 512, 5, 5]               0\n",
      "        MaxPool2d-28            [-1, 512, 2, 2]               0\n",
      "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-30            [-1, 512, 2, 2]               0\n",
      "           Conv2d-31            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-32            [-1, 512, 2, 2]               0\n",
      "           Conv2d-33            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-34            [-1, 512, 2, 2]               0\n",
      "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-36            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-37            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-38            [-1, 512, 7, 7]               0\n",
      "           Linear-39                 [-1, 4096]     102,764,544\n",
      "             ReLU-40                 [-1, 4096]               0\n",
      "          Dropout-41                 [-1, 4096]               0\n",
      "           Linear-42                 [-1, 4096]      16,781,312\n",
      "             ReLU-43                 [-1, 4096]               0\n",
      "          Dropout-44                 [-1, 4096]               0\n",
      "           Linear-45                 [-1, 1000]       4,097,000\n",
      "              VGG-46                 [-1, 1000]               0\n",
      "           Conv2d-47           [-1, 64, 45, 45]           1,792\n",
      "             ReLU-48           [-1, 64, 45, 45]               0\n",
      "           Conv2d-49           [-1, 64, 45, 45]          36,928\n",
      "             ReLU-50           [-1, 64, 45, 45]               0\n",
      "        MaxPool2d-51           [-1, 64, 22, 22]               0\n",
      "           Conv2d-52          [-1, 128, 22, 22]          73,856\n",
      "             ReLU-53          [-1, 128, 22, 22]               0\n",
      "           Conv2d-54          [-1, 128, 22, 22]         147,584\n",
      "             ReLU-55          [-1, 128, 22, 22]               0\n",
      "        MaxPool2d-56          [-1, 128, 11, 11]               0\n",
      "           Conv2d-57          [-1, 256, 11, 11]         295,168\n",
      "             ReLU-58          [-1, 256, 11, 11]               0\n",
      "           Conv2d-59          [-1, 256, 11, 11]         590,080\n",
      "             ReLU-60          [-1, 256, 11, 11]               0\n",
      "           Conv2d-61          [-1, 256, 11, 11]         590,080\n",
      "             ReLU-62          [-1, 256, 11, 11]               0\n",
      "           Conv2d-63          [-1, 256, 11, 11]         590,080\n",
      "             ReLU-64          [-1, 256, 11, 11]               0\n",
      "        MaxPool2d-65            [-1, 256, 5, 5]               0\n",
      "           Conv2d-66            [-1, 512, 5, 5]       1,180,160\n",
      "             ReLU-67            [-1, 512, 5, 5]               0\n",
      "           Conv2d-68            [-1, 512, 5, 5]       2,359,808\n",
      "             ReLU-69            [-1, 512, 5, 5]               0\n",
      "           Conv2d-70            [-1, 512, 5, 5]       2,359,808\n",
      "             ReLU-71            [-1, 512, 5, 5]               0\n",
      "           Conv2d-72            [-1, 512, 5, 5]       2,359,808\n",
      "             ReLU-73            [-1, 512, 5, 5]               0\n",
      "        MaxPool2d-74            [-1, 512, 2, 2]               0\n",
      "           Conv2d-75            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-76            [-1, 512, 2, 2]               0\n",
      "           Conv2d-77            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-78            [-1, 512, 2, 2]               0\n",
      "           Conv2d-79            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-80            [-1, 512, 2, 2]               0\n",
      "           Conv2d-81            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-82            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-83            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-84            [-1, 512, 7, 7]               0\n",
      "           Linear-85                 [-1, 4096]     102,764,544\n",
      "             ReLU-86                 [-1, 4096]               0\n",
      "          Dropout-87                 [-1, 4096]               0\n",
      "           Linear-88                 [-1, 4096]      16,781,312\n",
      "             ReLU-89                 [-1, 4096]               0\n",
      "          Dropout-90                 [-1, 4096]               0\n",
      "           Linear-91                 [-1, 1000]       4,097,000\n",
      "              VGG-92                 [-1, 1000]               0\n",
      "================================================================\n",
      "Total params: 287,334,480\n",
      "Trainable params: 287,334,480\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 18.92\n",
      "Params size (MB): 1096.09\n",
      "Estimated Total Size (MB): 1115.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(vgg19).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 45,45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(vgg19.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (6.9029) | Acc_1: (1.56%) (1/64)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (6.7920) | Acc_1: (19.89%) (140/704)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (6.4775) | Acc_1: (21.58%) (290/1344)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (6.1565) | Acc_1: (22.10%) (429/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (4.1933) | Acc: (19.42%) (20/103)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (3.9575) | Acc_1: (31.25%) (20/64)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (3.0909) | Acc_1: (28.55%) (201/704)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (2.3901) | Acc_1: (27.31%) (367/1344)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (2.0757) | Acc_1: (27.67%) (537/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3967) | Acc: (25.24%) (26/103)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (1.4131) | Acc_1: (20.31%) (13/64)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.4232) | Acc_1: (25.28%) (178/704)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.4148) | Acc_1: (26.12%) (351/1344)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.4122) | Acc_1: (26.94%) (523/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3932) | Acc: (25.24%) (26/103)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.3880) | Acc_1: (25.00%) (16/64)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (1.4101) | Acc_1: (26.28%) (185/704)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (1.4120) | Acc_1: (25.82%) (347/1344)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (1.4115) | Acc_1: (26.12%) (507/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3974) | Acc: (25.24%) (26/103)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (1.3828) | Acc_1: (26.56%) (17/64)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (1.4152) | Acc_1: (25.43%) (179/704)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (1.4219) | Acc_1: (24.48%) (329/1344)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (1.4166) | Acc_1: (25.19%) (489/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3902) | Acc: (30.10%) (31/103)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (1.3827) | Acc_1: (31.25%) (20/64)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (1.4083) | Acc_1: (26.85%) (189/704)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (1.4158) | Acc_1: (26.49%) (356/1344)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (1.4076) | Acc_1: (28.08%) (545/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3834) | Acc: (30.10%) (31/103)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (1.3705) | Acc_1: (34.38%) (22/64)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (1.4029) | Acc_1: (28.27%) (199/704)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (1.4086) | Acc_1: (26.79%) (360/1344)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (1.4058) | Acc_1: (26.48%) (514/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.4191) | Acc: (30.10%) (31/103)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (1.3986) | Acc_1: (29.69%) (19/64)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (1.4043) | Acc_1: (28.55%) (201/704)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (1.4009) | Acc_1: (27.60%) (371/1344)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (1.3984) | Acc_1: (27.77%) (539/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3905) | Acc: (30.10%) (31/103)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (1.3476) | Acc_1: (39.06%) (25/64)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (1.3862) | Acc_1: (29.40%) (207/704)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (1.3942) | Acc_1: (27.46%) (369/1344)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (1.3901) | Acc_1: (28.90%) (561/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3330) | Acc: (49.51%) (51/103)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (1.3982) | Acc_1: (25.00%) (16/64)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (1.3440) | Acc_1: (30.54%) (215/704)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (1.3204) | Acc_1: (35.42%) (476/1344)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (1.2654) | Acc_1: (39.21%) (761/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0353) | Acc: (48.54%) (50/103)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (1.0959) | Acc_1: (46.88%) (30/64)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (1.1580) | Acc_1: (46.02%) (324/704)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (1.1082) | Acc_1: (47.32%) (636/1344)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (1.0942) | Acc_1: (48.12%) (934/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9057) | Acc: (53.40%) (55/103)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (0.8569) | Acc_1: (62.50%) (40/64)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.9404) | Acc_1: (55.97%) (394/704)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (1.0207) | Acc_1: (53.20%) (715/1344)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (1.0062) | Acc_1: (52.91%) (1027/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1720) | Acc: (48.54%) (50/103)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (1.1823) | Acc_1: (45.31%) (29/64)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.9760) | Acc_1: (50.85%) (358/704)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.9309) | Acc_1: (55.58%) (747/1344)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.9183) | Acc_1: (55.49%) (1077/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8891) | Acc: (54.37%) (56/103)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.7802) | Acc_1: (59.38%) (38/64)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.8957) | Acc_1: (57.95%) (408/704)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.9654) | Acc_1: (54.76%) (736/1344)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.9525) | Acc_1: (55.23%) (1072/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9295) | Acc: (60.19%) (62/103)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.8422) | Acc_1: (57.81%) (37/64)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.9309) | Acc_1: (56.82%) (400/704)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.8887) | Acc_1: (57.81%) (777/1344)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.8692) | Acc_1: (58.01%) (1126/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9111) | Acc: (53.40%) (55/103)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.8104) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.9657) | Acc_1: (56.53%) (398/704)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.8856) | Acc_1: (59.15%) (795/1344)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.8816) | Acc_1: (59.35%) (1152/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8432) | Acc: (60.19%) (62/103)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.7230) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.8236) | Acc_1: (61.22%) (431/704)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.8391) | Acc_1: (60.12%) (808/1344)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.8407) | Acc_1: (60.28%) (1170/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8443) | Acc: (51.46%) (53/103)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.6917) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.8034) | Acc_1: (61.51%) (433/704)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.8422) | Acc_1: (59.67%) (802/1344)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.8237) | Acc_1: (60.38%) (1172/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7907) | Acc: (55.34%) (57/103)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.7455) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.6941) | Acc_1: (65.20%) (459/704)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.8084) | Acc_1: (62.35%) (838/1344)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.7860) | Acc_1: (62.70%) (1217/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1138) | Acc: (55.34%) (57/103)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.9847) | Acc_1: (62.50%) (40/64)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.7644) | Acc_1: (62.93%) (443/704)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.7232) | Acc_1: (64.06%) (861/1344)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.7330) | Acc_1: (64.14%) (1245/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8504) | Acc: (63.11%) (65/103)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.7101) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.7005) | Acc_1: (62.93%) (443/704)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.7513) | Acc_1: (63.54%) (854/1344)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.7336) | Acc_1: (64.40%) (1250/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8145) | Acc: (61.17%) (63/103)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.7440) | Acc_1: (62.50%) (40/64)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.6722) | Acc_1: (66.34%) (467/704)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.6995) | Acc_1: (65.40%) (879/1344)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.6943) | Acc_1: (65.64%) (1274/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8399) | Acc: (62.14%) (64/103)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.7276) | Acc_1: (57.81%) (37/64)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.7859) | Acc_1: (62.78%) (442/704)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.7447) | Acc_1: (64.58%) (868/1344)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.7357) | Acc_1: (65.17%) (1265/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7318) | Acc: (62.14%) (64/103)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.5031) | Acc_1: (71.88%) (46/64)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.6060) | Acc_1: (69.03%) (486/704)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.6222) | Acc_1: (68.82%) (925/1344)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.6503) | Acc_1: (68.26%) (1325/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9548) | Acc: (56.31%) (58/103)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.5583) | Acc_1: (71.88%) (46/64)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.8145) | Acc_1: (63.21%) (445/704)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.7275) | Acc_1: (67.34%) (905/1344)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.7371) | Acc_1: (66.36%) (1288/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7904) | Acc: (65.05%) (67/103)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.7208) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.7272) | Acc_1: (65.77%) (463/704)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.6706) | Acc_1: (68.15%) (916/1344)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.7242) | Acc_1: (66.82%) (1297/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7400) | Acc: (65.05%) (67/103)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.7133) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.6577) | Acc_1: (70.60%) (497/704)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.6304) | Acc_1: (71.58%) (962/1344)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.6524) | Acc_1: (70.32%) (1365/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7995) | Acc: (60.19%) (62/103)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.5535) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.5620) | Acc_1: (72.59%) (511/704)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.6135) | Acc_1: (70.83%) (952/1344)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.6201) | Acc_1: (70.74%) (1373/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6952) | Acc: (66.02%) (68/103)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.6678) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.6063) | Acc_1: (72.02%) (507/704)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.6029) | Acc_1: (73.07%) (982/1344)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.6100) | Acc_1: (72.49%) (1407/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7145) | Acc: (65.05%) (67/103)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.6167) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.5677) | Acc_1: (71.31%) (502/704)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.5980) | Acc_1: (70.09%) (942/1344)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.5918) | Acc_1: (71.30%) (1384/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8077) | Acc: (67.96%) (70/103)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.5067) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.6555) | Acc_1: (69.18%) (487/704)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.6466) | Acc_1: (71.21%) (957/1344)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.6498) | Acc_1: (70.63%) (1371/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7105) | Acc: (61.17%) (63/103)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.6629) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.5753) | Acc_1: (73.44%) (517/704)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.6160) | Acc_1: (71.73%) (964/1344)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.6000) | Acc_1: (72.33%) (1404/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7595) | Acc: (65.05%) (67/103)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.6462) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.6015) | Acc_1: (71.45%) (503/704)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.6018) | Acc_1: (72.02%) (968/1344)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.5899) | Acc_1: (72.23%) (1402/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7365) | Acc: (66.99%) (69/103)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.6077) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.5559) | Acc_1: (74.29%) (523/704)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.7096) | Acc_1: (70.54%) (948/1344)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.7932) | Acc_1: (66.72%) (1295/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8830) | Acc: (57.28%) (59/103)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.7387) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.7222) | Acc_1: (66.48%) (468/704)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.6804) | Acc_1: (68.45%) (920/1344)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.6758) | Acc_1: (68.62%) (1332/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7477) | Acc: (65.05%) (67/103)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.6245) | Acc_1: (71.88%) (46/64)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.6023) | Acc_1: (71.59%) (504/704)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.6141) | Acc_1: (71.73%) (964/1344)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.6286) | Acc_1: (71.46%) (1387/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7926) | Acc: (61.17%) (63/103)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.8154) | Acc_1: (60.94%) (39/64)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.6091) | Acc_1: (71.45%) (503/704)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.6068) | Acc_1: (71.43%) (960/1344)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.5950) | Acc_1: (72.28%) (1403/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8197) | Acc: (66.02%) (68/103)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.5372) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.5665) | Acc_1: (76.42%) (538/704)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.5383) | Acc_1: (76.26%) (1025/1344)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.5549) | Acc_1: (75.53%) (1466/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7412) | Acc: (66.99%) (69/103)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.6817) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.5743) | Acc_1: (73.30%) (516/704)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.6139) | Acc_1: (72.47%) (974/1344)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.6010) | Acc_1: (73.16%) (1420/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7507) | Acc: (62.14%) (64/103)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.5800) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.5852) | Acc_1: (72.16%) (508/704)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.5897) | Acc_1: (73.88%) (993/1344)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.5944) | Acc_1: (73.00%) (1417/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7022) | Acc: (66.99%) (69/103)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.5541) | Acc_1: (71.88%) (46/64)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.5571) | Acc_1: (74.01%) (521/704)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.5489) | Acc_1: (74.26%) (998/1344)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.5603) | Acc_1: (73.93%) (1435/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.6426) | Acc: (45.63%) (47/103)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (1.2478) | Acc_1: (51.56%) (33/64)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.8591) | Acc_1: (62.50%) (440/704)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.7245) | Acc_1: (67.11%) (902/1344)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.6683) | Acc_1: (69.86%) (1356/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7812) | Acc: (66.99%) (69/103)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.5996) | Acc_1: (71.88%) (46/64)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.5838) | Acc_1: (72.02%) (507/704)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.5724) | Acc_1: (72.92%) (980/1344)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.5714) | Acc_1: (72.95%) (1416/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7609) | Acc: (64.08%) (66/103)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.5495) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.5521) | Acc_1: (74.57%) (525/704)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.5335) | Acc_1: (75.52%) (1015/1344)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.5437) | Acc_1: (74.65%) (1449/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7535) | Acc: (66.02%) (68/103)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.4364) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.5022) | Acc_1: (75.99%) (535/704)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.4961) | Acc_1: (76.34%) (1026/1344)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.5042) | Acc_1: (76.20%) (1479/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.2489) | Acc: (47.57%) (49/103)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (1.8249) | Acc_1: (53.12%) (34/64)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.8909) | Acc_1: (64.06%) (451/704)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.7502) | Acc_1: (68.23%) (917/1344)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.6943) | Acc_1: (70.17%) (1362/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7557) | Acc: (67.96%) (70/103)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.5876) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.5548) | Acc_1: (73.44%) (517/704)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.5748) | Acc_1: (72.77%) (978/1344)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.5462) | Acc_1: (74.65%) (1449/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6798) | Acc: (62.14%) (64/103)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.4938) | Acc_1: (82.81%) (53/64)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.4928) | Acc_1: (77.41%) (545/704)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.5223) | Acc_1: (76.04%) (1022/1344)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.5414) | Acc_1: (75.12%) (1458/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7805) | Acc: (66.02%) (68/103)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.4471) | Acc_1: (79.69%) (51/64)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.4763) | Acc_1: (76.56%) (539/704)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.5112) | Acc_1: (74.85%) (1006/1344)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.5119) | Acc_1: (75.01%) (1456/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7888) | Acc: (64.08%) (66/103)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.6740) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.6140) | Acc_1: (70.74%) (498/704)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.5578) | Acc_1: (73.51%) (988/1344)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.5324) | Acc_1: (74.50%) (1446/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7419) | Acc: (63.11%) (65/103)\n",
      "2 hours 26 mins 30 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='vgg19.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 50):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='vgg19.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_img(img_path, model):\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # opencv는 BGR순서로 read한다.\n",
    "    img = cv2.resize(img, (45, 45))\n",
    "    img = torch.from_numpy(img).float()\n",
    "\n",
    "    img = img.permute(2, 0, 1).squeeze(0) # (H, W, C) -> (C, H, W) -> (1, C, H, W)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.Softmax()\n",
    "\n",
    "    out = model(img)\n",
    "    label_idx = torch.argmax(out, dim=1)\n",
    "    prob = criterion(out)\n",
    "\n",
    "    return prob, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/hom1/ict01/.conda/envs/tomato_cls/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "0 hours 0 mins 7 secs for training\n"
     ]
    }
   ],
   "source": [
    "default_directory = './save_models'\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='vgg19.tar.gz')\n",
    "\n",
    "# model편에서 만들어 두었던 MyNetwork를 활용.\n",
    "\n",
    "model = models.vgg19()\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "#model.eval()\n",
    "\n",
    "start = time.time()\n",
    "i=0\n",
    "for img in glob.iglob('./data/test/**/*.jpg', recursive=True):\n",
    "    i+=1\n",
    "    pred, label_idx = classification_img(img, model)\n",
    "\n",
    "now = time.gmtime(time.time() - start)\n",
    "print(i)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea228a1e57bab7175f6663bb7b03ad0c48b776b0d8531c957bf1be729535464c"
  },
  "kernelspec": {
   "display_name": "tomato_cls",
   "language": "python",
   "name": "tomato_cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
