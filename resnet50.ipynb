{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1,2,3,4,5'\n",
    "start_time = time.time()\n",
    "batch_size = 64\n",
    "learning_rate = 0.003\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = datasets.ImageFolder(\"./data/train\",\n",
    "                         transform=transforms.Compose([transforms.Resize(64),\n",
    "                                                       transforms.RandomCrop(45),\n",
    "                                                       transforms.ToTensor()]))\n",
    "\n",
    "test_imgs = datasets.ImageFolder(\"./data/test\",\n",
    "                        transform=transforms.Compose([transforms.Resize(64),\n",
    "                                                      transforms.RandomCrop(45),\n",
    "                                                      transforms.ToTensor()]))\n",
    "\n",
    "train_loader = DataLoader(train_imgs, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_imgs, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 5 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 23, 23]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 23, 23]             128\n",
      "              ReLU-3           [-1, 64, 23, 23]               0\n",
      "         MaxPool2d-4           [-1, 64, 12, 12]               0\n",
      "            Conv2d-5           [-1, 64, 12, 12]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 12, 12]             128\n",
      "              ReLU-7           [-1, 64, 12, 12]               0\n",
      "            Conv2d-8           [-1, 64, 12, 12]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 12, 12]             128\n",
      "             ReLU-10           [-1, 64, 12, 12]               0\n",
      "           Conv2d-11          [-1, 256, 12, 12]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 12, 12]             512\n",
      "           Conv2d-13          [-1, 256, 12, 12]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 12, 12]             512\n",
      "             ReLU-15          [-1, 256, 12, 12]               0\n",
      "       Bottleneck-16          [-1, 256, 12, 12]               0\n",
      "           Conv2d-17           [-1, 64, 12, 12]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 12, 12]             128\n",
      "             ReLU-19           [-1, 64, 12, 12]               0\n",
      "           Conv2d-20           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 12, 12]             128\n",
      "             ReLU-22           [-1, 64, 12, 12]               0\n",
      "           Conv2d-23          [-1, 256, 12, 12]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 12, 12]             512\n",
      "             ReLU-25          [-1, 256, 12, 12]               0\n",
      "       Bottleneck-26          [-1, 256, 12, 12]               0\n",
      "           Conv2d-27           [-1, 64, 12, 12]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 12, 12]             128\n",
      "             ReLU-29           [-1, 64, 12, 12]               0\n",
      "           Conv2d-30           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 12, 12]             128\n",
      "             ReLU-32           [-1, 64, 12, 12]               0\n",
      "           Conv2d-33          [-1, 256, 12, 12]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 12, 12]             512\n",
      "             ReLU-35          [-1, 256, 12, 12]               0\n",
      "       Bottleneck-36          [-1, 256, 12, 12]               0\n",
      "           Conv2d-37          [-1, 128, 12, 12]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 12, 12]             256\n",
      "             ReLU-39          [-1, 128, 12, 12]               0\n",
      "           Conv2d-40            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-41            [-1, 128, 6, 6]             256\n",
      "             ReLU-42            [-1, 128, 6, 6]               0\n",
      "           Conv2d-43            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-44            [-1, 512, 6, 6]           1,024\n",
      "           Conv2d-45            [-1, 512, 6, 6]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-47            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-48            [-1, 512, 6, 6]               0\n",
      "           Conv2d-49            [-1, 128, 6, 6]          65,536\n",
      "      BatchNorm2d-50            [-1, 128, 6, 6]             256\n",
      "             ReLU-51            [-1, 128, 6, 6]               0\n",
      "           Conv2d-52            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 6, 6]             256\n",
      "             ReLU-54            [-1, 128, 6, 6]               0\n",
      "           Conv2d-55            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-56            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-57            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-58            [-1, 512, 6, 6]               0\n",
      "           Conv2d-59            [-1, 128, 6, 6]          65,536\n",
      "      BatchNorm2d-60            [-1, 128, 6, 6]             256\n",
      "             ReLU-61            [-1, 128, 6, 6]               0\n",
      "           Conv2d-62            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 6, 6]             256\n",
      "             ReLU-64            [-1, 128, 6, 6]               0\n",
      "           Conv2d-65            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-66            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-67            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-68            [-1, 512, 6, 6]               0\n",
      "           Conv2d-69            [-1, 128, 6, 6]          65,536\n",
      "      BatchNorm2d-70            [-1, 128, 6, 6]             256\n",
      "             ReLU-71            [-1, 128, 6, 6]               0\n",
      "           Conv2d-72            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 6, 6]             256\n",
      "             ReLU-74            [-1, 128, 6, 6]               0\n",
      "           Conv2d-75            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-76            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-77            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-78            [-1, 512, 6, 6]               0\n",
      "           Conv2d-79            [-1, 256, 6, 6]         131,072\n",
      "      BatchNorm2d-80            [-1, 256, 6, 6]             512\n",
      "             ReLU-81            [-1, 256, 6, 6]               0\n",
      "           Conv2d-82            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 3, 3]             512\n",
      "             ReLU-84            [-1, 256, 3, 3]               0\n",
      "           Conv2d-85           [-1, 1024, 3, 3]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 3, 3]           2,048\n",
      "           Conv2d-87           [-1, 1024, 3, 3]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 3, 3]           2,048\n",
      "             ReLU-89           [-1, 1024, 3, 3]               0\n",
      "       Bottleneck-90           [-1, 1024, 3, 3]               0\n",
      "           Conv2d-91            [-1, 256, 3, 3]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 3, 3]             512\n",
      "             ReLU-93            [-1, 256, 3, 3]               0\n",
      "           Conv2d-94            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 3, 3]             512\n",
      "             ReLU-96            [-1, 256, 3, 3]               0\n",
      "           Conv2d-97           [-1, 1024, 3, 3]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 3, 3]           2,048\n",
      "             ReLU-99           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-100           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-101            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 3, 3]             512\n",
      "            ReLU-103            [-1, 256, 3, 3]               0\n",
      "          Conv2d-104            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 3, 3]             512\n",
      "            ReLU-106            [-1, 256, 3, 3]               0\n",
      "          Conv2d-107           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-109           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-110           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-111            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 3, 3]             512\n",
      "            ReLU-113            [-1, 256, 3, 3]               0\n",
      "          Conv2d-114            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 3, 3]             512\n",
      "            ReLU-116            [-1, 256, 3, 3]               0\n",
      "          Conv2d-117           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-119           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-120           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-121            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 3, 3]             512\n",
      "            ReLU-123            [-1, 256, 3, 3]               0\n",
      "          Conv2d-124            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 3, 3]             512\n",
      "            ReLU-126            [-1, 256, 3, 3]               0\n",
      "          Conv2d-127           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-129           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-130           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-131            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 3, 3]             512\n",
      "            ReLU-133            [-1, 256, 3, 3]               0\n",
      "          Conv2d-134            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 3, 3]             512\n",
      "            ReLU-136            [-1, 256, 3, 3]               0\n",
      "          Conv2d-137           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-139           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-140           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-141            [-1, 512, 3, 3]         524,288\n",
      "     BatchNorm2d-142            [-1, 512, 3, 3]           1,024\n",
      "            ReLU-143            [-1, 512, 3, 3]               0\n",
      "          Conv2d-144            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-146            [-1, 512, 2, 2]               0\n",
      "          Conv2d-147           [-1, 2048, 2, 2]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 2, 2]           4,096\n",
      "          Conv2d-149           [-1, 2048, 2, 2]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 2, 2]           4,096\n",
      "            ReLU-151           [-1, 2048, 2, 2]               0\n",
      "      Bottleneck-152           [-1, 2048, 2, 2]               0\n",
      "          Conv2d-153            [-1, 512, 2, 2]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-155            [-1, 512, 2, 2]               0\n",
      "          Conv2d-156            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-158            [-1, 512, 2, 2]               0\n",
      "          Conv2d-159           [-1, 2048, 2, 2]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 2, 2]           4,096\n",
      "            ReLU-161           [-1, 2048, 2, 2]               0\n",
      "      Bottleneck-162           [-1, 2048, 2, 2]               0\n",
      "          Conv2d-163            [-1, 512, 2, 2]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-165            [-1, 512, 2, 2]               0\n",
      "          Conv2d-166            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-168            [-1, 512, 2, 2]               0\n",
      "          Conv2d-169           [-1, 2048, 2, 2]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 2, 2]           4,096\n",
      "            ReLU-171           [-1, 2048, 2, 2]               0\n",
      "      Bottleneck-172           [-1, 2048, 2, 2]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "          ResNet-175                 [-1, 1000]               0\n",
      "          Conv2d-176           [-1, 64, 23, 23]           9,408\n",
      "     BatchNorm2d-177           [-1, 64, 23, 23]             128\n",
      "            ReLU-178           [-1, 64, 23, 23]               0\n",
      "       MaxPool2d-179           [-1, 64, 12, 12]               0\n",
      "          Conv2d-180           [-1, 64, 12, 12]           4,096\n",
      "     BatchNorm2d-181           [-1, 64, 12, 12]             128\n",
      "            ReLU-182           [-1, 64, 12, 12]               0\n",
      "          Conv2d-183           [-1, 64, 12, 12]          36,864\n",
      "     BatchNorm2d-184           [-1, 64, 12, 12]             128\n",
      "            ReLU-185           [-1, 64, 12, 12]               0\n",
      "          Conv2d-186          [-1, 256, 12, 12]          16,384\n",
      "     BatchNorm2d-187          [-1, 256, 12, 12]             512\n",
      "          Conv2d-188          [-1, 256, 12, 12]          16,384\n",
      "     BatchNorm2d-189          [-1, 256, 12, 12]             512\n",
      "            ReLU-190          [-1, 256, 12, 12]               0\n",
      "      Bottleneck-191          [-1, 256, 12, 12]               0\n",
      "          Conv2d-192           [-1, 64, 12, 12]          16,384\n",
      "     BatchNorm2d-193           [-1, 64, 12, 12]             128\n",
      "            ReLU-194           [-1, 64, 12, 12]               0\n",
      "          Conv2d-195           [-1, 64, 12, 12]          36,864\n",
      "     BatchNorm2d-196           [-1, 64, 12, 12]             128\n",
      "            ReLU-197           [-1, 64, 12, 12]               0\n",
      "          Conv2d-198          [-1, 256, 12, 12]          16,384\n",
      "     BatchNorm2d-199          [-1, 256, 12, 12]             512\n",
      "            ReLU-200          [-1, 256, 12, 12]               0\n",
      "      Bottleneck-201          [-1, 256, 12, 12]               0\n",
      "          Conv2d-202           [-1, 64, 12, 12]          16,384\n",
      "     BatchNorm2d-203           [-1, 64, 12, 12]             128\n",
      "            ReLU-204           [-1, 64, 12, 12]               0\n",
      "          Conv2d-205           [-1, 64, 12, 12]          36,864\n",
      "     BatchNorm2d-206           [-1, 64, 12, 12]             128\n",
      "            ReLU-207           [-1, 64, 12, 12]               0\n",
      "          Conv2d-208          [-1, 256, 12, 12]          16,384\n",
      "     BatchNorm2d-209          [-1, 256, 12, 12]             512\n",
      "            ReLU-210          [-1, 256, 12, 12]               0\n",
      "      Bottleneck-211          [-1, 256, 12, 12]               0\n",
      "          Conv2d-212          [-1, 128, 12, 12]          32,768\n",
      "     BatchNorm2d-213          [-1, 128, 12, 12]             256\n",
      "            ReLU-214          [-1, 128, 12, 12]               0\n",
      "          Conv2d-215            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-216            [-1, 128, 6, 6]             256\n",
      "            ReLU-217            [-1, 128, 6, 6]               0\n",
      "          Conv2d-218            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-219            [-1, 512, 6, 6]           1,024\n",
      "          Conv2d-220            [-1, 512, 6, 6]         131,072\n",
      "     BatchNorm2d-221            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-222            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-223            [-1, 512, 6, 6]               0\n",
      "          Conv2d-224            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-225            [-1, 128, 6, 6]             256\n",
      "            ReLU-226            [-1, 128, 6, 6]               0\n",
      "          Conv2d-227            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-228            [-1, 128, 6, 6]             256\n",
      "            ReLU-229            [-1, 128, 6, 6]               0\n",
      "          Conv2d-230            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-231            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-232            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-233            [-1, 512, 6, 6]               0\n",
      "          Conv2d-234            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-235            [-1, 128, 6, 6]             256\n",
      "            ReLU-236            [-1, 128, 6, 6]               0\n",
      "          Conv2d-237            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-238            [-1, 128, 6, 6]             256\n",
      "            ReLU-239            [-1, 128, 6, 6]               0\n",
      "          Conv2d-240            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-241            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-242            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-243            [-1, 512, 6, 6]               0\n",
      "          Conv2d-244            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-245            [-1, 128, 6, 6]             256\n",
      "            ReLU-246            [-1, 128, 6, 6]               0\n",
      "          Conv2d-247            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-248            [-1, 128, 6, 6]             256\n",
      "            ReLU-249            [-1, 128, 6, 6]               0\n",
      "          Conv2d-250            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-251            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-252            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-253            [-1, 512, 6, 6]               0\n",
      "          Conv2d-254            [-1, 256, 6, 6]         131,072\n",
      "     BatchNorm2d-255            [-1, 256, 6, 6]             512\n",
      "            ReLU-256            [-1, 256, 6, 6]               0\n",
      "          Conv2d-257            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-258            [-1, 256, 3, 3]             512\n",
      "            ReLU-259            [-1, 256, 3, 3]               0\n",
      "          Conv2d-260           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-261           [-1, 1024, 3, 3]           2,048\n",
      "          Conv2d-262           [-1, 1024, 3, 3]         524,288\n",
      "     BatchNorm2d-263           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-264           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-265           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-266            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-267            [-1, 256, 3, 3]             512\n",
      "            ReLU-268            [-1, 256, 3, 3]               0\n",
      "          Conv2d-269            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-270            [-1, 256, 3, 3]             512\n",
      "            ReLU-271            [-1, 256, 3, 3]               0\n",
      "          Conv2d-272           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-273           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-274           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-275           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-276            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-277            [-1, 256, 3, 3]             512\n",
      "            ReLU-278            [-1, 256, 3, 3]               0\n",
      "          Conv2d-279            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-280            [-1, 256, 3, 3]             512\n",
      "            ReLU-281            [-1, 256, 3, 3]               0\n",
      "          Conv2d-282           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-283           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-284           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-285           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-286            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-287            [-1, 256, 3, 3]             512\n",
      "            ReLU-288            [-1, 256, 3, 3]               0\n",
      "          Conv2d-289            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-290            [-1, 256, 3, 3]             512\n",
      "            ReLU-291            [-1, 256, 3, 3]               0\n",
      "          Conv2d-292           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-293           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-294           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-295           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-296            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-297            [-1, 256, 3, 3]             512\n",
      "            ReLU-298            [-1, 256, 3, 3]               0\n",
      "          Conv2d-299            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-300            [-1, 256, 3, 3]             512\n",
      "            ReLU-301            [-1, 256, 3, 3]               0\n",
      "          Conv2d-302           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-303           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-304           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-305           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-306            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-307            [-1, 256, 3, 3]             512\n",
      "            ReLU-308            [-1, 256, 3, 3]               0\n",
      "          Conv2d-309            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-310            [-1, 256, 3, 3]             512\n",
      "            ReLU-311            [-1, 256, 3, 3]               0\n",
      "          Conv2d-312           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-313           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-314           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-315           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-316            [-1, 512, 3, 3]         524,288\n",
      "     BatchNorm2d-317            [-1, 512, 3, 3]           1,024\n",
      "            ReLU-318            [-1, 512, 3, 3]               0\n",
      "          Conv2d-319            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-320            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-321            [-1, 512, 2, 2]               0\n",
      "          Conv2d-322           [-1, 2048, 2, 2]       1,048,576\n",
      "     BatchNorm2d-323           [-1, 2048, 2, 2]           4,096\n",
      "          Conv2d-324           [-1, 2048, 2, 2]       2,097,152\n",
      "     BatchNorm2d-325           [-1, 2048, 2, 2]           4,096\n",
      "            ReLU-326           [-1, 2048, 2, 2]               0\n",
      "      Bottleneck-327           [-1, 2048, 2, 2]               0\n",
      "          Conv2d-328            [-1, 512, 2, 2]       1,048,576\n",
      "     BatchNorm2d-329            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-330            [-1, 512, 2, 2]               0\n",
      "          Conv2d-331            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-332            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-333            [-1, 512, 2, 2]               0\n",
      "          Conv2d-334           [-1, 2048, 2, 2]       1,048,576\n",
      "     BatchNorm2d-335           [-1, 2048, 2, 2]           4,096\n",
      "            ReLU-336           [-1, 2048, 2, 2]               0\n",
      "      Bottleneck-337           [-1, 2048, 2, 2]               0\n",
      "          Conv2d-338            [-1, 512, 2, 2]       1,048,576\n",
      "     BatchNorm2d-339            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-340            [-1, 512, 2, 2]               0\n",
      "          Conv2d-341            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-342            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-343            [-1, 512, 2, 2]               0\n",
      "          Conv2d-344           [-1, 2048, 2, 2]       1,048,576\n",
      "     BatchNorm2d-345           [-1, 2048, 2, 2]           4,096\n",
      "            ReLU-346           [-1, 2048, 2, 2]               0\n",
      "      Bottleneck-347           [-1, 2048, 2, 2]               0\n",
      "AdaptiveAvgPool2d-348           [-1, 2048, 1, 1]               0\n",
      "          Linear-349                 [-1, 1000]       2,049,000\n",
      "          ResNet-350                 [-1, 1000]               0\n",
      "================================================================\n",
      "Total params: 51,114,064\n",
      "Trainable params: 51,114,064\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 27.21\n",
      "Params size (MB): 194.98\n",
      "Estimated Total Size (MB): 222.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(resnet50).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 45,45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(resnet50.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (7.2043) | Acc_1: (0.00%) (0/64)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (3.1481) | Acc_1: (22.87%) (161/704)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (2.5821) | Acc_1: (26.93%) (362/1344)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (2.2993) | Acc_1: (29.01%) (563/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (48.7682) | Acc: (30.10%) (31/103)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (1.4761) | Acc_1: (51.56%) (33/64)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (1.3732) | Acc_1: (44.03%) (310/704)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (1.4118) | Acc_1: (43.90%) (590/1344)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (1.4067) | Acc_1: (43.59%) (846/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (5.7248) | Acc: (42.72%) (44/103)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (1.9562) | Acc_1: (42.19%) (27/64)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.3097) | Acc_1: (50.99%) (359/704)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.2849) | Acc_1: (52.08%) (700/1344)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.1929) | Acc_1: (53.74%) (1043/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0410) | Acc: (61.17%) (63/103)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.0354) | Acc_1: (50.00%) (32/64)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (0.9868) | Acc_1: (57.53%) (405/704)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (0.9786) | Acc_1: (57.37%) (771/1344)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (0.9606) | Acc_1: (59.35%) (1152/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7940) | Acc: (66.02%) (68/103)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (1.0959) | Acc_1: (59.38%) (38/64)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (0.9658) | Acc_1: (59.80%) (421/704)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (0.9207) | Acc_1: (61.09%) (821/1344)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (0.8754) | Acc_1: (62.96%) (1222/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0252) | Acc: (65.05%) (67/103)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (0.9738) | Acc_1: (57.81%) (37/64)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (0.9071) | Acc_1: (62.36%) (439/704)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (0.8369) | Acc_1: (66.07%) (888/1344)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (0.8474) | Acc_1: (65.79%) (1277/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8819) | Acc: (58.25%) (60/103)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (0.6888) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (0.7981) | Acc_1: (65.77%) (463/704)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (0.8072) | Acc_1: (66.29%) (891/1344)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (0.8202) | Acc_1: (67.95%) (1319/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.2034) | Acc: (54.37%) (56/103)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (0.7215) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (0.8178) | Acc_1: (66.05%) (465/704)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (0.8339) | Acc_1: (67.04%) (901/1344)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (0.8032) | Acc_1: (68.01%) (1320/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1724) | Acc: (59.22%) (61/103)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (1.4345) | Acc_1: (59.38%) (38/64)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (0.7535) | Acc_1: (71.73%) (505/704)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (0.7299) | Acc_1: (71.43%) (960/1344)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (0.7414) | Acc_1: (70.43%) (1367/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8131) | Acc: (66.02%) (68/103)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (0.8585) | Acc_1: (71.88%) (46/64)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (0.7343) | Acc_1: (71.45%) (503/704)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (0.7325) | Acc_1: (71.21%) (957/1344)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (0.7354) | Acc_1: (71.41%) (1386/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.2244) | Acc: (58.25%) (60/103)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (0.7961) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (0.5653) | Acc_1: (76.42%) (538/704)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (0.6095) | Acc_1: (75.07%) (1009/1344)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (0.6106) | Acc_1: (74.86%) (1453/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8844) | Acc: (71.84%) (74/103)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (0.6742) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.6417) | Acc_1: (73.30%) (516/704)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (0.6106) | Acc_1: (75.00%) (1008/1344)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (0.5897) | Acc_1: (76.82%) (1491/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.6327) | Acc: (44.66%) (46/103)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (1.4872) | Acc_1: (37.50%) (24/64)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.8361) | Acc_1: (70.60%) (497/704)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.6868) | Acc_1: (74.93%) (1007/1344)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.6435) | Acc_1: (75.63%) (1468/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9486) | Acc: (73.79%) (76/103)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.7910) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.5540) | Acc_1: (77.41%) (545/704)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.5382) | Acc_1: (78.35%) (1053/1344)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.5641) | Acc_1: (78.21%) (1518/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.9478) | Acc: (56.31%) (58/103)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (1.2396) | Acc_1: (62.50%) (40/64)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.6276) | Acc_1: (75.00%) (528/704)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.5809) | Acc_1: (76.41%) (1027/1344)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.5776) | Acc_1: (76.82%) (1491/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6623) | Acc: (78.64%) (81/103)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.3897) | Acc_1: (84.38%) (54/64)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.4409) | Acc_1: (81.82%) (576/704)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.4769) | Acc_1: (81.55%) (1096/1344)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.5076) | Acc_1: (81.04%) (1573/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6550) | Acc: (72.82%) (75/103)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.4316) | Acc_1: (84.38%) (54/64)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.6269) | Acc_1: (77.13%) (543/704)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.5441) | Acc_1: (79.17%) (1064/1344)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.5526) | Acc_1: (79.29%) (1539/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8393) | Acc: (76.70%) (79/103)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (1.0429) | Acc_1: (78.12%) (50/64)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.5940) | Acc_1: (79.55%) (560/704)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.4941) | Acc_1: (82.14%) (1104/1344)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.4818) | Acc_1: (82.43%) (1600/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.1941) | Acc: (44.66%) (46/103)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.6023) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.4818) | Acc_1: (82.95%) (584/704)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.4484) | Acc_1: (84.15%) (1131/1344)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.4349) | Acc_1: (84.03%) (1631/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5693) | Acc: (75.73%) (78/103)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.5083) | Acc_1: (84.38%) (54/64)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.4783) | Acc_1: (81.53%) (574/704)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.4447) | Acc_1: (83.18%) (1118/1344)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.4389) | Acc_1: (83.51%) (1621/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4734) | Acc: (77.67%) (80/103)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.3936) | Acc_1: (85.94%) (55/64)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.3763) | Acc_1: (84.94%) (598/704)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.3858) | Acc_1: (85.49%) (1149/1344)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.3758) | Acc_1: (85.68%) (1663/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9968) | Acc: (64.08%) (66/103)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.6316) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.4011) | Acc_1: (81.96%) (577/704)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.3802) | Acc_1: (84.38%) (1134/1344)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.4252) | Acc_1: (84.13%) (1633/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (4.3407) | Acc: (58.25%) (60/103)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.6488) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.4580) | Acc_1: (84.94%) (598/704)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.4558) | Acc_1: (84.00%) (1129/1344)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.4441) | Acc_1: (84.96%) (1649/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9147) | Acc: (69.90%) (72/103)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.3117) | Acc_1: (89.06%) (57/64)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.3245) | Acc_1: (86.65%) (610/704)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.3178) | Acc_1: (86.68%) (1165/1344)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.3559) | Acc_1: (86.14%) (1672/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4674) | Acc: (80.58%) (83/103)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.2309) | Acc_1: (93.75%) (60/64)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.3367) | Acc_1: (86.51%) (609/704)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.3563) | Acc_1: (85.34%) (1147/1344)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.3458) | Acc_1: (86.30%) (1675/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3445) | Acc: (87.38%) (90/103)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.4122) | Acc_1: (82.81%) (53/64)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.3448) | Acc_1: (86.93%) (612/704)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.3362) | Acc_1: (87.28%) (1173/1344)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.3554) | Acc_1: (86.55%) (1680/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3328) | Acc: (85.44%) (88/103)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.3688) | Acc_1: (90.62%) (58/64)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.3109) | Acc_1: (88.21%) (621/704)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.3278) | Acc_1: (87.95%) (1182/1344)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.3072) | Acc_1: (88.56%) (1719/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4854) | Acc: (86.41%) (89/103)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.2274) | Acc_1: (93.75%) (60/64)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.3344) | Acc_1: (87.50%) (616/704)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.3104) | Acc_1: (88.32%) (1187/1344)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.3520) | Acc_1: (87.48%) (1698/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6172) | Acc: (75.73%) (78/103)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.3984) | Acc_1: (82.81%) (53/64)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.3217) | Acc_1: (86.08%) (606/704)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.3960) | Acc_1: (85.04%) (1143/1344)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.4183) | Acc_1: (85.83%) (1666/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4356) | Acc: (82.52%) (85/103)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.3404) | Acc_1: (84.38%) (54/64)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.3087) | Acc_1: (88.49%) (623/704)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.2943) | Acc_1: (88.84%) (1194/1344)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.2879) | Acc_1: (89.18%) (1731/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5455) | Acc: (77.67%) (80/103)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.3191) | Acc_1: (87.50%) (56/64)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.2989) | Acc_1: (88.64%) (624/704)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.2974) | Acc_1: (89.21%) (1199/1344)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.2928) | Acc_1: (89.18%) (1731/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3678) | Acc: (83.50%) (86/103)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.2851) | Acc_1: (90.62%) (58/64)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.2542) | Acc_1: (90.62%) (638/704)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.2643) | Acc_1: (90.10%) (1211/1344)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.2754) | Acc_1: (90.06%) (1748/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2498) | Acc: (88.35%) (91/103)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.2935) | Acc_1: (90.62%) (58/64)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.2883) | Acc_1: (90.62%) (638/704)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.2686) | Acc_1: (89.96%) (1209/1344)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.2938) | Acc_1: (89.64%) (1740/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3799) | Acc: (82.52%) (85/103)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.2947) | Acc_1: (87.50%) (56/64)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.3658) | Acc_1: (86.79%) (611/704)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.3040) | Acc_1: (89.66%) (1205/1344)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.2887) | Acc_1: (89.75%) (1742/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7748) | Acc: (76.70%) (79/103)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.3172) | Acc_1: (85.94%) (55/64)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.2859) | Acc_1: (89.77%) (632/704)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.2922) | Acc_1: (89.29%) (1200/1344)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.2966) | Acc_1: (89.59%) (1739/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5043) | Acc: (72.82%) (75/103)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.1816) | Acc_1: (95.31%) (61/64)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.2213) | Acc_1: (92.19%) (649/704)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.2454) | Acc_1: (91.52%) (1230/1344)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.2648) | Acc_1: (90.98%) (1766/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.5493) | Acc: (80.58%) (83/103)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.2959) | Acc_1: (89.06%) (57/64)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.3603) | Acc_1: (88.21%) (621/704)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.3342) | Acc_1: (88.99%) (1196/1344)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.2992) | Acc_1: (89.70%) (1741/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5075) | Acc: (79.61%) (82/103)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.2999) | Acc_1: (90.62%) (58/64)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.2839) | Acc_1: (90.91%) (640/704)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.2352) | Acc_1: (92.11%) (1238/1344)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.2565) | Acc_1: (92.48%) (1795/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.5076) | Acc: (73.79%) (76/103)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.2570) | Acc_1: (92.19%) (59/64)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.1973) | Acc_1: (92.90%) (654/704)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.2014) | Acc_1: (92.78%) (1247/1344)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.2101) | Acc_1: (92.74%) (1800/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3784) | Acc: (84.47%) (87/103)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.1940) | Acc_1: (92.19%) (59/64)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.2047) | Acc_1: (93.18%) (656/704)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.2383) | Acc_1: (91.82%) (1234/1344)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.2455) | Acc_1: (91.81%) (1782/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4602) | Acc: (84.47%) (87/103)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.3087) | Acc_1: (90.62%) (58/64)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.2646) | Acc_1: (89.91%) (633/704)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.2272) | Acc_1: (91.37%) (1228/1344)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.2165) | Acc_1: (91.91%) (1784/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2083) | Acc: (92.23%) (95/103)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.4657) | Acc_1: (82.81%) (53/64)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.2450) | Acc_1: (92.76%) (653/704)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.2344) | Acc_1: (92.41%) (1242/1344)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.2306) | Acc_1: (92.43%) (1794/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.1197) | Acc: (96.12%) (99/103)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.3970) | Acc_1: (81.25%) (52/64)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.1690) | Acc_1: (94.46%) (665/704)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.1583) | Acc_1: (94.72%) (1273/1344)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.1892) | Acc_1: (93.97%) (1824/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6492) | Acc: (79.61%) (82/103)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.1447) | Acc_1: (93.75%) (60/64)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.1702) | Acc_1: (93.47%) (658/704)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.2072) | Acc_1: (92.78%) (1247/1344)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.2189) | Acc_1: (92.58%) (1797/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3258) | Acc: (86.41%) (89/103)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.0872) | Acc_1: (96.88%) (62/64)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.2141) | Acc_1: (92.05%) (648/704)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.2146) | Acc_1: (92.11%) (1238/1344)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.2073) | Acc_1: (92.68%) (1799/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2748) | Acc: (91.26%) (94/103)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.1907) | Acc_1: (93.75%) (60/64)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.2286) | Acc_1: (92.05%) (648/704)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.2000) | Acc_1: (93.23%) (1253/1344)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.2012) | Acc_1: (93.25%) (1810/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3016) | Acc: (86.41%) (89/103)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.1626) | Acc_1: (92.19%) (59/64)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.2295) | Acc_1: (92.47%) (651/704)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.2030) | Acc_1: (93.38%) (1255/1344)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.2235) | Acc_1: (93.82%) (1821/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4381) | Acc: (83.50%) (86/103)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.1876) | Acc_1: (93.75%) (60/64)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.2568) | Acc_1: (90.62%) (638/704)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.2232) | Acc_1: (91.82%) (1234/1344)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.2076) | Acc_1: (92.74%) (1800/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4150) | Acc: (90.29%) (93/103)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.2659) | Acc_1: (89.06%) (57/64)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.1831) | Acc_1: (93.32%) (657/704)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.1536) | Acc_1: (94.42%) (1269/1344)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.1592) | Acc_1: (94.64%) (1837/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2051) | Acc: (92.23%) (95/103)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.1678) | Acc_1: (92.19%) (59/64)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.1794) | Acc_1: (93.75%) (660/704)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.1673) | Acc_1: (93.82%) (1261/1344)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.2005) | Acc_1: (93.71%) (1819/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2804) | Acc: (90.29%) (93/103)\n",
      "2 hours 18 mins 40 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet50.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 50):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='resnet50.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_img(img_path, model):\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # opencv는 BGR순서로 read한다.\n",
    "    img = cv2.resize(img, (45, 45))\n",
    "    img = torch.from_numpy(img).float()\n",
    "\n",
    "    img = img.permute(2, 0, 1).squeeze(0) # (H, W, C) -> (C, H, W) -> (1, C, H, W)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "    print(img.size())\n",
    "\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.Softmax()\n",
    "\n",
    "    out = model(img)\n",
    "    label_idx = torch.argmax(out, dim=1)\n",
    "    prob = criterion(out)\n",
    "\n",
    "    return prob, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "torch.Size([1, 3, 45, 45])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/hom1/ict01/.conda/envs/tomato_cls/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "torch.Size([1, 3, 45, 45])\n",
      "0 hours 0 mins 9 secs for training\n"
     ]
    }
   ],
   "source": [
    "default_directory = './save_models'\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet50.tar.gz')\n",
    "\n",
    "# model편에서 만들어 두었던 MyNetwork를 활용.\n",
    "\n",
    "model = models.resnet50()\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "#model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for img in glob.iglob('./data/test/**/*.jpg', recursive=True):\n",
    "    pred, label_idx = classification_img(img, model)\n",
    "\n",
    "now = time.gmtime(time.time() - start)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea228a1e57bab7175f6663bb7b03ad0c48b776b0d8531c957bf1be729535464c"
  },
  "kernelspec": {
   "display_name": "tomato_cls",
   "language": "python",
   "name": "tomato_cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
