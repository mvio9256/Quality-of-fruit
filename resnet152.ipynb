{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1,2,3,4,5'\n",
    "start_time = time.time()\n",
    "batch_size = 64\n",
    "learning_rate = 0.003\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/resnet152')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = datasets.ImageFolder(\"./data/train\",\n",
    "                         transform=transforms.Compose([transforms.Resize(64),\n",
    "                                                       transforms.RandomCrop(45),\n",
    "                                                       transforms.ToTensor()]))\n",
    "\n",
    "test_imgs = datasets.ImageFolder(\"./data/test\",\n",
    "                        transform=transforms.Compose([transforms.Resize(64),\n",
    "                                                      transforms.RandomCrop(45),\n",
    "                                                      transforms.ToTensor()]))\n",
    "\n",
    "train_loader = DataLoader(train_imgs, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_imgs, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152 = models.resnet152()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 5 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 23, 23]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 23, 23]             128\n",
      "              ReLU-3           [-1, 64, 23, 23]               0\n",
      "         MaxPool2d-4           [-1, 64, 12, 12]               0\n",
      "            Conv2d-5           [-1, 64, 12, 12]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 12, 12]             128\n",
      "              ReLU-7           [-1, 64, 12, 12]               0\n",
      "            Conv2d-8           [-1, 64, 12, 12]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 12, 12]             128\n",
      "             ReLU-10           [-1, 64, 12, 12]               0\n",
      "           Conv2d-11          [-1, 256, 12, 12]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 12, 12]             512\n",
      "           Conv2d-13          [-1, 256, 12, 12]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 12, 12]             512\n",
      "             ReLU-15          [-1, 256, 12, 12]               0\n",
      "       Bottleneck-16          [-1, 256, 12, 12]               0\n",
      "           Conv2d-17           [-1, 64, 12, 12]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 12, 12]             128\n",
      "             ReLU-19           [-1, 64, 12, 12]               0\n",
      "           Conv2d-20           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 12, 12]             128\n",
      "             ReLU-22           [-1, 64, 12, 12]               0\n",
      "           Conv2d-23          [-1, 256, 12, 12]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 12, 12]             512\n",
      "             ReLU-25          [-1, 256, 12, 12]               0\n",
      "       Bottleneck-26          [-1, 256, 12, 12]               0\n",
      "           Conv2d-27           [-1, 64, 12, 12]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 12, 12]             128\n",
      "             ReLU-29           [-1, 64, 12, 12]               0\n",
      "           Conv2d-30           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 12, 12]             128\n",
      "             ReLU-32           [-1, 64, 12, 12]               0\n",
      "           Conv2d-33          [-1, 256, 12, 12]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 12, 12]             512\n",
      "             ReLU-35          [-1, 256, 12, 12]               0\n",
      "       Bottleneck-36          [-1, 256, 12, 12]               0\n",
      "           Conv2d-37          [-1, 128, 12, 12]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 12, 12]             256\n",
      "             ReLU-39          [-1, 128, 12, 12]               0\n",
      "           Conv2d-40            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-41            [-1, 128, 6, 6]             256\n",
      "             ReLU-42            [-1, 128, 6, 6]               0\n",
      "           Conv2d-43            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-44            [-1, 512, 6, 6]           1,024\n",
      "           Conv2d-45            [-1, 512, 6, 6]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-47            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-48            [-1, 512, 6, 6]               0\n",
      "           Conv2d-49            [-1, 128, 6, 6]          65,536\n",
      "      BatchNorm2d-50            [-1, 128, 6, 6]             256\n",
      "             ReLU-51            [-1, 128, 6, 6]               0\n",
      "           Conv2d-52            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 6, 6]             256\n",
      "             ReLU-54            [-1, 128, 6, 6]               0\n",
      "           Conv2d-55            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-56            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-57            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-58            [-1, 512, 6, 6]               0\n",
      "           Conv2d-59            [-1, 128, 6, 6]          65,536\n",
      "      BatchNorm2d-60            [-1, 128, 6, 6]             256\n",
      "             ReLU-61            [-1, 128, 6, 6]               0\n",
      "           Conv2d-62            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 6, 6]             256\n",
      "             ReLU-64            [-1, 128, 6, 6]               0\n",
      "           Conv2d-65            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-66            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-67            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-68            [-1, 512, 6, 6]               0\n",
      "           Conv2d-69            [-1, 128, 6, 6]          65,536\n",
      "      BatchNorm2d-70            [-1, 128, 6, 6]             256\n",
      "             ReLU-71            [-1, 128, 6, 6]               0\n",
      "           Conv2d-72            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 6, 6]             256\n",
      "             ReLU-74            [-1, 128, 6, 6]               0\n",
      "           Conv2d-75            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-76            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-77            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-78            [-1, 512, 6, 6]               0\n",
      "           Conv2d-79            [-1, 128, 6, 6]          65,536\n",
      "      BatchNorm2d-80            [-1, 128, 6, 6]             256\n",
      "             ReLU-81            [-1, 128, 6, 6]               0\n",
      "           Conv2d-82            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-83            [-1, 128, 6, 6]             256\n",
      "             ReLU-84            [-1, 128, 6, 6]               0\n",
      "           Conv2d-85            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-86            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-87            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-88            [-1, 512, 6, 6]               0\n",
      "           Conv2d-89            [-1, 128, 6, 6]          65,536\n",
      "      BatchNorm2d-90            [-1, 128, 6, 6]             256\n",
      "             ReLU-91            [-1, 128, 6, 6]               0\n",
      "           Conv2d-92            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-93            [-1, 128, 6, 6]             256\n",
      "             ReLU-94            [-1, 128, 6, 6]               0\n",
      "           Conv2d-95            [-1, 512, 6, 6]          65,536\n",
      "      BatchNorm2d-96            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-97            [-1, 512, 6, 6]               0\n",
      "       Bottleneck-98            [-1, 512, 6, 6]               0\n",
      "           Conv2d-99            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-100            [-1, 128, 6, 6]             256\n",
      "            ReLU-101            [-1, 128, 6, 6]               0\n",
      "          Conv2d-102            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-103            [-1, 128, 6, 6]             256\n",
      "            ReLU-104            [-1, 128, 6, 6]               0\n",
      "          Conv2d-105            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-106            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-107            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-108            [-1, 512, 6, 6]               0\n",
      "          Conv2d-109            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-110            [-1, 128, 6, 6]             256\n",
      "            ReLU-111            [-1, 128, 6, 6]               0\n",
      "          Conv2d-112            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-113            [-1, 128, 6, 6]             256\n",
      "            ReLU-114            [-1, 128, 6, 6]               0\n",
      "          Conv2d-115            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-116            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-117            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-118            [-1, 512, 6, 6]               0\n",
      "          Conv2d-119            [-1, 256, 6, 6]         131,072\n",
      "     BatchNorm2d-120            [-1, 256, 6, 6]             512\n",
      "            ReLU-121            [-1, 256, 6, 6]               0\n",
      "          Conv2d-122            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-123            [-1, 256, 3, 3]             512\n",
      "            ReLU-124            [-1, 256, 3, 3]               0\n",
      "          Conv2d-125           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-126           [-1, 1024, 3, 3]           2,048\n",
      "          Conv2d-127           [-1, 1024, 3, 3]         524,288\n",
      "     BatchNorm2d-128           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-129           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-130           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-131            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 3, 3]             512\n",
      "            ReLU-133            [-1, 256, 3, 3]               0\n",
      "          Conv2d-134            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 3, 3]             512\n",
      "            ReLU-136            [-1, 256, 3, 3]               0\n",
      "          Conv2d-137           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-139           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-140           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-141            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-142            [-1, 256, 3, 3]             512\n",
      "            ReLU-143            [-1, 256, 3, 3]               0\n",
      "          Conv2d-144            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-145            [-1, 256, 3, 3]             512\n",
      "            ReLU-146            [-1, 256, 3, 3]               0\n",
      "          Conv2d-147           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-148           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-149           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-150           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-151            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-152            [-1, 256, 3, 3]             512\n",
      "            ReLU-153            [-1, 256, 3, 3]               0\n",
      "          Conv2d-154            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-155            [-1, 256, 3, 3]             512\n",
      "            ReLU-156            [-1, 256, 3, 3]               0\n",
      "          Conv2d-157           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-158           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-159           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-160           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-161            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-162            [-1, 256, 3, 3]             512\n",
      "            ReLU-163            [-1, 256, 3, 3]               0\n",
      "          Conv2d-164            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-165            [-1, 256, 3, 3]             512\n",
      "            ReLU-166            [-1, 256, 3, 3]               0\n",
      "          Conv2d-167           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-168           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-169           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-170           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-171            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-172            [-1, 256, 3, 3]             512\n",
      "            ReLU-173            [-1, 256, 3, 3]               0\n",
      "          Conv2d-174            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-175            [-1, 256, 3, 3]             512\n",
      "            ReLU-176            [-1, 256, 3, 3]               0\n",
      "          Conv2d-177           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-178           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-179           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-180           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-181            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-182            [-1, 256, 3, 3]             512\n",
      "            ReLU-183            [-1, 256, 3, 3]               0\n",
      "          Conv2d-184            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-185            [-1, 256, 3, 3]             512\n",
      "            ReLU-186            [-1, 256, 3, 3]               0\n",
      "          Conv2d-187           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-188           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-189           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-190           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-191            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-192            [-1, 256, 3, 3]             512\n",
      "            ReLU-193            [-1, 256, 3, 3]               0\n",
      "          Conv2d-194            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-195            [-1, 256, 3, 3]             512\n",
      "            ReLU-196            [-1, 256, 3, 3]               0\n",
      "          Conv2d-197           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-198           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-199           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-200           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-201            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-202            [-1, 256, 3, 3]             512\n",
      "            ReLU-203            [-1, 256, 3, 3]               0\n",
      "          Conv2d-204            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-205            [-1, 256, 3, 3]             512\n",
      "            ReLU-206            [-1, 256, 3, 3]               0\n",
      "          Conv2d-207           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-208           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-209           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-210           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-211            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-212            [-1, 256, 3, 3]             512\n",
      "            ReLU-213            [-1, 256, 3, 3]               0\n",
      "          Conv2d-214            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-215            [-1, 256, 3, 3]             512\n",
      "            ReLU-216            [-1, 256, 3, 3]               0\n",
      "          Conv2d-217           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-218           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-219           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-220           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-221            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-222            [-1, 256, 3, 3]             512\n",
      "            ReLU-223            [-1, 256, 3, 3]               0\n",
      "          Conv2d-224            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-225            [-1, 256, 3, 3]             512\n",
      "            ReLU-226            [-1, 256, 3, 3]               0\n",
      "          Conv2d-227           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-228           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-229           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-230           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-231            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-232            [-1, 256, 3, 3]             512\n",
      "            ReLU-233            [-1, 256, 3, 3]               0\n",
      "          Conv2d-234            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-235            [-1, 256, 3, 3]             512\n",
      "            ReLU-236            [-1, 256, 3, 3]               0\n",
      "          Conv2d-237           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-238           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-239           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-240           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-241            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-242            [-1, 256, 3, 3]             512\n",
      "            ReLU-243            [-1, 256, 3, 3]               0\n",
      "          Conv2d-244            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-245            [-1, 256, 3, 3]             512\n",
      "            ReLU-246            [-1, 256, 3, 3]               0\n",
      "          Conv2d-247           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-248           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-249           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-250           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-251            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-252            [-1, 256, 3, 3]             512\n",
      "            ReLU-253            [-1, 256, 3, 3]               0\n",
      "          Conv2d-254            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-255            [-1, 256, 3, 3]             512\n",
      "            ReLU-256            [-1, 256, 3, 3]               0\n",
      "          Conv2d-257           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-258           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-259           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-260           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-261            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-262            [-1, 256, 3, 3]             512\n",
      "            ReLU-263            [-1, 256, 3, 3]               0\n",
      "          Conv2d-264            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-265            [-1, 256, 3, 3]             512\n",
      "            ReLU-266            [-1, 256, 3, 3]               0\n",
      "          Conv2d-267           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-268           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-269           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-270           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-271            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-272            [-1, 256, 3, 3]             512\n",
      "            ReLU-273            [-1, 256, 3, 3]               0\n",
      "          Conv2d-274            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-275            [-1, 256, 3, 3]             512\n",
      "            ReLU-276            [-1, 256, 3, 3]               0\n",
      "          Conv2d-277           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-278           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-279           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-280           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-281            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-282            [-1, 256, 3, 3]             512\n",
      "            ReLU-283            [-1, 256, 3, 3]               0\n",
      "          Conv2d-284            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-285            [-1, 256, 3, 3]             512\n",
      "            ReLU-286            [-1, 256, 3, 3]               0\n",
      "          Conv2d-287           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-288           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-289           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-290           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-291            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-292            [-1, 256, 3, 3]             512\n",
      "            ReLU-293            [-1, 256, 3, 3]               0\n",
      "          Conv2d-294            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-295            [-1, 256, 3, 3]             512\n",
      "            ReLU-296            [-1, 256, 3, 3]               0\n",
      "          Conv2d-297           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-298           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-299           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-300           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-301            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-302            [-1, 256, 3, 3]             512\n",
      "            ReLU-303            [-1, 256, 3, 3]               0\n",
      "          Conv2d-304            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-305            [-1, 256, 3, 3]             512\n",
      "            ReLU-306            [-1, 256, 3, 3]               0\n",
      "          Conv2d-307           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-308           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-309           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-310           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-311            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-312            [-1, 256, 3, 3]             512\n",
      "            ReLU-313            [-1, 256, 3, 3]               0\n",
      "          Conv2d-314            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-315            [-1, 256, 3, 3]             512\n",
      "            ReLU-316            [-1, 256, 3, 3]               0\n",
      "          Conv2d-317           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-318           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-319           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-320           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-321            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-322            [-1, 256, 3, 3]             512\n",
      "            ReLU-323            [-1, 256, 3, 3]               0\n",
      "          Conv2d-324            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-325            [-1, 256, 3, 3]             512\n",
      "            ReLU-326            [-1, 256, 3, 3]               0\n",
      "          Conv2d-327           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-328           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-329           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-330           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-331            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-332            [-1, 256, 3, 3]             512\n",
      "            ReLU-333            [-1, 256, 3, 3]               0\n",
      "          Conv2d-334            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-335            [-1, 256, 3, 3]             512\n",
      "            ReLU-336            [-1, 256, 3, 3]               0\n",
      "          Conv2d-337           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-338           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-339           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-340           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-341            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-342            [-1, 256, 3, 3]             512\n",
      "            ReLU-343            [-1, 256, 3, 3]               0\n",
      "          Conv2d-344            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-345            [-1, 256, 3, 3]             512\n",
      "            ReLU-346            [-1, 256, 3, 3]               0\n",
      "          Conv2d-347           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-348           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-349           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-350           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-351            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-352            [-1, 256, 3, 3]             512\n",
      "            ReLU-353            [-1, 256, 3, 3]               0\n",
      "          Conv2d-354            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-355            [-1, 256, 3, 3]             512\n",
      "            ReLU-356            [-1, 256, 3, 3]               0\n",
      "          Conv2d-357           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-358           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-359           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-360           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-361            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-362            [-1, 256, 3, 3]             512\n",
      "            ReLU-363            [-1, 256, 3, 3]               0\n",
      "          Conv2d-364            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-365            [-1, 256, 3, 3]             512\n",
      "            ReLU-366            [-1, 256, 3, 3]               0\n",
      "          Conv2d-367           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-368           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-369           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-370           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-371            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-372            [-1, 256, 3, 3]             512\n",
      "            ReLU-373            [-1, 256, 3, 3]               0\n",
      "          Conv2d-374            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-375            [-1, 256, 3, 3]             512\n",
      "            ReLU-376            [-1, 256, 3, 3]               0\n",
      "          Conv2d-377           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-378           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-379           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-380           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-381            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-382            [-1, 256, 3, 3]             512\n",
      "            ReLU-383            [-1, 256, 3, 3]               0\n",
      "          Conv2d-384            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-385            [-1, 256, 3, 3]             512\n",
      "            ReLU-386            [-1, 256, 3, 3]               0\n",
      "          Conv2d-387           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-388           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-389           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-390           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-391            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-392            [-1, 256, 3, 3]             512\n",
      "            ReLU-393            [-1, 256, 3, 3]               0\n",
      "          Conv2d-394            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-395            [-1, 256, 3, 3]             512\n",
      "            ReLU-396            [-1, 256, 3, 3]               0\n",
      "          Conv2d-397           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-398           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-399           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-400           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-401            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-402            [-1, 256, 3, 3]             512\n",
      "            ReLU-403            [-1, 256, 3, 3]               0\n",
      "          Conv2d-404            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-405            [-1, 256, 3, 3]             512\n",
      "            ReLU-406            [-1, 256, 3, 3]               0\n",
      "          Conv2d-407           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-408           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-409           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-410           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-411            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-412            [-1, 256, 3, 3]             512\n",
      "            ReLU-413            [-1, 256, 3, 3]               0\n",
      "          Conv2d-414            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-415            [-1, 256, 3, 3]             512\n",
      "            ReLU-416            [-1, 256, 3, 3]               0\n",
      "          Conv2d-417           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-418           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-419           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-420           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-421            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-422            [-1, 256, 3, 3]             512\n",
      "            ReLU-423            [-1, 256, 3, 3]               0\n",
      "          Conv2d-424            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-425            [-1, 256, 3, 3]             512\n",
      "            ReLU-426            [-1, 256, 3, 3]               0\n",
      "          Conv2d-427           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-428           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-429           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-430           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-431            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-432            [-1, 256, 3, 3]             512\n",
      "            ReLU-433            [-1, 256, 3, 3]               0\n",
      "          Conv2d-434            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-435            [-1, 256, 3, 3]             512\n",
      "            ReLU-436            [-1, 256, 3, 3]               0\n",
      "          Conv2d-437           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-438           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-439           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-440           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-441            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-442            [-1, 256, 3, 3]             512\n",
      "            ReLU-443            [-1, 256, 3, 3]               0\n",
      "          Conv2d-444            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-445            [-1, 256, 3, 3]             512\n",
      "            ReLU-446            [-1, 256, 3, 3]               0\n",
      "          Conv2d-447           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-448           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-449           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-450           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-451            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-452            [-1, 256, 3, 3]             512\n",
      "            ReLU-453            [-1, 256, 3, 3]               0\n",
      "          Conv2d-454            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-455            [-1, 256, 3, 3]             512\n",
      "            ReLU-456            [-1, 256, 3, 3]               0\n",
      "          Conv2d-457           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-458           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-459           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-460           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-461            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-462            [-1, 256, 3, 3]             512\n",
      "            ReLU-463            [-1, 256, 3, 3]               0\n",
      "          Conv2d-464            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-465            [-1, 256, 3, 3]             512\n",
      "            ReLU-466            [-1, 256, 3, 3]               0\n",
      "          Conv2d-467           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-468           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-469           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-470           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-471            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-472            [-1, 256, 3, 3]             512\n",
      "            ReLU-473            [-1, 256, 3, 3]               0\n",
      "          Conv2d-474            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-475            [-1, 256, 3, 3]             512\n",
      "            ReLU-476            [-1, 256, 3, 3]               0\n",
      "          Conv2d-477           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-478           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-479           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-480           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-481            [-1, 512, 3, 3]         524,288\n",
      "     BatchNorm2d-482            [-1, 512, 3, 3]           1,024\n",
      "            ReLU-483            [-1, 512, 3, 3]               0\n",
      "          Conv2d-484            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-485            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-486            [-1, 512, 2, 2]               0\n",
      "          Conv2d-487           [-1, 2048, 2, 2]       1,048,576\n",
      "     BatchNorm2d-488           [-1, 2048, 2, 2]           4,096\n",
      "          Conv2d-489           [-1, 2048, 2, 2]       2,097,152\n",
      "     BatchNorm2d-490           [-1, 2048, 2, 2]           4,096\n",
      "            ReLU-491           [-1, 2048, 2, 2]               0\n",
      "      Bottleneck-492           [-1, 2048, 2, 2]               0\n",
      "          Conv2d-493            [-1, 512, 2, 2]       1,048,576\n",
      "     BatchNorm2d-494            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-495            [-1, 512, 2, 2]               0\n",
      "          Conv2d-496            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-497            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-498            [-1, 512, 2, 2]               0\n",
      "          Conv2d-499           [-1, 2048, 2, 2]       1,048,576\n",
      "     BatchNorm2d-500           [-1, 2048, 2, 2]           4,096\n",
      "            ReLU-501           [-1, 2048, 2, 2]               0\n",
      "      Bottleneck-502           [-1, 2048, 2, 2]               0\n",
      "          Conv2d-503            [-1, 512, 2, 2]       1,048,576\n",
      "     BatchNorm2d-504            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-505            [-1, 512, 2, 2]               0\n",
      "          Conv2d-506            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-507            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-508            [-1, 512, 2, 2]               0\n",
      "          Conv2d-509           [-1, 2048, 2, 2]       1,048,576\n",
      "     BatchNorm2d-510           [-1, 2048, 2, 2]           4,096\n",
      "            ReLU-511           [-1, 2048, 2, 2]               0\n",
      "      Bottleneck-512           [-1, 2048, 2, 2]               0\n",
      "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
      "          Linear-514                 [-1, 1000]       2,049,000\n",
      "          ResNet-515                 [-1, 1000]               0\n",
      "          Conv2d-516           [-1, 64, 23, 23]           9,408\n",
      "     BatchNorm2d-517           [-1, 64, 23, 23]             128\n",
      "            ReLU-518           [-1, 64, 23, 23]               0\n",
      "       MaxPool2d-519           [-1, 64, 12, 12]               0\n",
      "          Conv2d-520           [-1, 64, 12, 12]           4,096\n",
      "     BatchNorm2d-521           [-1, 64, 12, 12]             128\n",
      "            ReLU-522           [-1, 64, 12, 12]               0\n",
      "          Conv2d-523           [-1, 64, 12, 12]          36,864\n",
      "     BatchNorm2d-524           [-1, 64, 12, 12]             128\n",
      "            ReLU-525           [-1, 64, 12, 12]               0\n",
      "          Conv2d-526          [-1, 256, 12, 12]          16,384\n",
      "     BatchNorm2d-527          [-1, 256, 12, 12]             512\n",
      "          Conv2d-528          [-1, 256, 12, 12]          16,384\n",
      "     BatchNorm2d-529          [-1, 256, 12, 12]             512\n",
      "            ReLU-530          [-1, 256, 12, 12]               0\n",
      "      Bottleneck-531          [-1, 256, 12, 12]               0\n",
      "          Conv2d-532           [-1, 64, 12, 12]          16,384\n",
      "     BatchNorm2d-533           [-1, 64, 12, 12]             128\n",
      "            ReLU-534           [-1, 64, 12, 12]               0\n",
      "          Conv2d-535           [-1, 64, 12, 12]          36,864\n",
      "     BatchNorm2d-536           [-1, 64, 12, 12]             128\n",
      "            ReLU-537           [-1, 64, 12, 12]               0\n",
      "          Conv2d-538          [-1, 256, 12, 12]          16,384\n",
      "     BatchNorm2d-539          [-1, 256, 12, 12]             512\n",
      "            ReLU-540          [-1, 256, 12, 12]               0\n",
      "      Bottleneck-541          [-1, 256, 12, 12]               0\n",
      "          Conv2d-542           [-1, 64, 12, 12]          16,384\n",
      "     BatchNorm2d-543           [-1, 64, 12, 12]             128\n",
      "            ReLU-544           [-1, 64, 12, 12]               0\n",
      "          Conv2d-545           [-1, 64, 12, 12]          36,864\n",
      "     BatchNorm2d-546           [-1, 64, 12, 12]             128\n",
      "            ReLU-547           [-1, 64, 12, 12]               0\n",
      "          Conv2d-548          [-1, 256, 12, 12]          16,384\n",
      "     BatchNorm2d-549          [-1, 256, 12, 12]             512\n",
      "            ReLU-550          [-1, 256, 12, 12]               0\n",
      "      Bottleneck-551          [-1, 256, 12, 12]               0\n",
      "          Conv2d-552          [-1, 128, 12, 12]          32,768\n",
      "     BatchNorm2d-553          [-1, 128, 12, 12]             256\n",
      "            ReLU-554          [-1, 128, 12, 12]               0\n",
      "          Conv2d-555            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-556            [-1, 128, 6, 6]             256\n",
      "            ReLU-557            [-1, 128, 6, 6]               0\n",
      "          Conv2d-558            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-559            [-1, 512, 6, 6]           1,024\n",
      "          Conv2d-560            [-1, 512, 6, 6]         131,072\n",
      "     BatchNorm2d-561            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-562            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-563            [-1, 512, 6, 6]               0\n",
      "          Conv2d-564            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-565            [-1, 128, 6, 6]             256\n",
      "            ReLU-566            [-1, 128, 6, 6]               0\n",
      "          Conv2d-567            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-568            [-1, 128, 6, 6]             256\n",
      "            ReLU-569            [-1, 128, 6, 6]               0\n",
      "          Conv2d-570            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-571            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-572            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-573            [-1, 512, 6, 6]               0\n",
      "          Conv2d-574            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-575            [-1, 128, 6, 6]             256\n",
      "            ReLU-576            [-1, 128, 6, 6]               0\n",
      "          Conv2d-577            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-578            [-1, 128, 6, 6]             256\n",
      "            ReLU-579            [-1, 128, 6, 6]               0\n",
      "          Conv2d-580            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-581            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-582            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-583            [-1, 512, 6, 6]               0\n",
      "          Conv2d-584            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-585            [-1, 128, 6, 6]             256\n",
      "            ReLU-586            [-1, 128, 6, 6]               0\n",
      "          Conv2d-587            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-588            [-1, 128, 6, 6]             256\n",
      "            ReLU-589            [-1, 128, 6, 6]               0\n",
      "          Conv2d-590            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-591            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-592            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-593            [-1, 512, 6, 6]               0\n",
      "          Conv2d-594            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-595            [-1, 128, 6, 6]             256\n",
      "            ReLU-596            [-1, 128, 6, 6]               0\n",
      "          Conv2d-597            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-598            [-1, 128, 6, 6]             256\n",
      "            ReLU-599            [-1, 128, 6, 6]               0\n",
      "          Conv2d-600            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-601            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-602            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-603            [-1, 512, 6, 6]               0\n",
      "          Conv2d-604            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-605            [-1, 128, 6, 6]             256\n",
      "            ReLU-606            [-1, 128, 6, 6]               0\n",
      "          Conv2d-607            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-608            [-1, 128, 6, 6]             256\n",
      "            ReLU-609            [-1, 128, 6, 6]               0\n",
      "          Conv2d-610            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-611            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-612            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-613            [-1, 512, 6, 6]               0\n",
      "          Conv2d-614            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-615            [-1, 128, 6, 6]             256\n",
      "            ReLU-616            [-1, 128, 6, 6]               0\n",
      "          Conv2d-617            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-618            [-1, 128, 6, 6]             256\n",
      "            ReLU-619            [-1, 128, 6, 6]               0\n",
      "          Conv2d-620            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-621            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-622            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-623            [-1, 512, 6, 6]               0\n",
      "          Conv2d-624            [-1, 128, 6, 6]          65,536\n",
      "     BatchNorm2d-625            [-1, 128, 6, 6]             256\n",
      "            ReLU-626            [-1, 128, 6, 6]               0\n",
      "          Conv2d-627            [-1, 128, 6, 6]         147,456\n",
      "     BatchNorm2d-628            [-1, 128, 6, 6]             256\n",
      "            ReLU-629            [-1, 128, 6, 6]               0\n",
      "          Conv2d-630            [-1, 512, 6, 6]          65,536\n",
      "     BatchNorm2d-631            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-632            [-1, 512, 6, 6]               0\n",
      "      Bottleneck-633            [-1, 512, 6, 6]               0\n",
      "          Conv2d-634            [-1, 256, 6, 6]         131,072\n",
      "     BatchNorm2d-635            [-1, 256, 6, 6]             512\n",
      "            ReLU-636            [-1, 256, 6, 6]               0\n",
      "          Conv2d-637            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-638            [-1, 256, 3, 3]             512\n",
      "            ReLU-639            [-1, 256, 3, 3]               0\n",
      "          Conv2d-640           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-641           [-1, 1024, 3, 3]           2,048\n",
      "          Conv2d-642           [-1, 1024, 3, 3]         524,288\n",
      "     BatchNorm2d-643           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-644           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-645           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-646            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-647            [-1, 256, 3, 3]             512\n",
      "            ReLU-648            [-1, 256, 3, 3]               0\n",
      "          Conv2d-649            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-650            [-1, 256, 3, 3]             512\n",
      "            ReLU-651            [-1, 256, 3, 3]               0\n",
      "          Conv2d-652           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-653           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-654           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-655           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-656            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-657            [-1, 256, 3, 3]             512\n",
      "            ReLU-658            [-1, 256, 3, 3]               0\n",
      "          Conv2d-659            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-660            [-1, 256, 3, 3]             512\n",
      "            ReLU-661            [-1, 256, 3, 3]               0\n",
      "          Conv2d-662           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-663           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-664           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-665           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-666            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-667            [-1, 256, 3, 3]             512\n",
      "            ReLU-668            [-1, 256, 3, 3]               0\n",
      "          Conv2d-669            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-670            [-1, 256, 3, 3]             512\n",
      "            ReLU-671            [-1, 256, 3, 3]               0\n",
      "          Conv2d-672           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-673           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-674           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-675           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-676            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-677            [-1, 256, 3, 3]             512\n",
      "            ReLU-678            [-1, 256, 3, 3]               0\n",
      "          Conv2d-679            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-680            [-1, 256, 3, 3]             512\n",
      "            ReLU-681            [-1, 256, 3, 3]               0\n",
      "          Conv2d-682           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-683           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-684           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-685           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-686            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-687            [-1, 256, 3, 3]             512\n",
      "            ReLU-688            [-1, 256, 3, 3]               0\n",
      "          Conv2d-689            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-690            [-1, 256, 3, 3]             512\n",
      "            ReLU-691            [-1, 256, 3, 3]               0\n",
      "          Conv2d-692           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-693           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-694           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-695           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-696            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-697            [-1, 256, 3, 3]             512\n",
      "            ReLU-698            [-1, 256, 3, 3]               0\n",
      "          Conv2d-699            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-700            [-1, 256, 3, 3]             512\n",
      "            ReLU-701            [-1, 256, 3, 3]               0\n",
      "          Conv2d-702           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-703           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-704           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-705           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-706            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-707            [-1, 256, 3, 3]             512\n",
      "            ReLU-708            [-1, 256, 3, 3]               0\n",
      "          Conv2d-709            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-710            [-1, 256, 3, 3]             512\n",
      "            ReLU-711            [-1, 256, 3, 3]               0\n",
      "          Conv2d-712           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-713           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-714           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-715           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-716            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-717            [-1, 256, 3, 3]             512\n",
      "            ReLU-718            [-1, 256, 3, 3]               0\n",
      "          Conv2d-719            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-720            [-1, 256, 3, 3]             512\n",
      "            ReLU-721            [-1, 256, 3, 3]               0\n",
      "          Conv2d-722           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-723           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-724           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-725           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-726            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-727            [-1, 256, 3, 3]             512\n",
      "            ReLU-728            [-1, 256, 3, 3]               0\n",
      "          Conv2d-729            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-730            [-1, 256, 3, 3]             512\n",
      "            ReLU-731            [-1, 256, 3, 3]               0\n",
      "          Conv2d-732           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-733           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-734           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-735           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-736            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-737            [-1, 256, 3, 3]             512\n",
      "            ReLU-738            [-1, 256, 3, 3]               0\n",
      "          Conv2d-739            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-740            [-1, 256, 3, 3]             512\n",
      "            ReLU-741            [-1, 256, 3, 3]               0\n",
      "          Conv2d-742           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-743           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-744           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-745           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-746            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-747            [-1, 256, 3, 3]             512\n",
      "            ReLU-748            [-1, 256, 3, 3]               0\n",
      "          Conv2d-749            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-750            [-1, 256, 3, 3]             512\n",
      "            ReLU-751            [-1, 256, 3, 3]               0\n",
      "          Conv2d-752           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-753           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-754           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-755           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-756            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-757            [-1, 256, 3, 3]             512\n",
      "            ReLU-758            [-1, 256, 3, 3]               0\n",
      "          Conv2d-759            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-760            [-1, 256, 3, 3]             512\n",
      "            ReLU-761            [-1, 256, 3, 3]               0\n",
      "          Conv2d-762           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-763           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-764           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-765           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-766            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-767            [-1, 256, 3, 3]             512\n",
      "            ReLU-768            [-1, 256, 3, 3]               0\n",
      "          Conv2d-769            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-770            [-1, 256, 3, 3]             512\n",
      "            ReLU-771            [-1, 256, 3, 3]               0\n",
      "          Conv2d-772           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-773           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-774           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-775           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-776            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-777            [-1, 256, 3, 3]             512\n",
      "            ReLU-778            [-1, 256, 3, 3]               0\n",
      "          Conv2d-779            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-780            [-1, 256, 3, 3]             512\n",
      "            ReLU-781            [-1, 256, 3, 3]               0\n",
      "          Conv2d-782           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-783           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-784           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-785           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-786            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-787            [-1, 256, 3, 3]             512\n",
      "            ReLU-788            [-1, 256, 3, 3]               0\n",
      "          Conv2d-789            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-790            [-1, 256, 3, 3]             512\n",
      "            ReLU-791            [-1, 256, 3, 3]               0\n",
      "          Conv2d-792           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-793           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-794           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-795           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-796            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-797            [-1, 256, 3, 3]             512\n",
      "            ReLU-798            [-1, 256, 3, 3]               0\n",
      "          Conv2d-799            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-800            [-1, 256, 3, 3]             512\n",
      "            ReLU-801            [-1, 256, 3, 3]               0\n",
      "          Conv2d-802           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-803           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-804           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-805           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-806            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-807            [-1, 256, 3, 3]             512\n",
      "            ReLU-808            [-1, 256, 3, 3]               0\n",
      "          Conv2d-809            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-810            [-1, 256, 3, 3]             512\n",
      "            ReLU-811            [-1, 256, 3, 3]               0\n",
      "          Conv2d-812           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-813           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-814           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-815           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-816            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-817            [-1, 256, 3, 3]             512\n",
      "            ReLU-818            [-1, 256, 3, 3]               0\n",
      "          Conv2d-819            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-820            [-1, 256, 3, 3]             512\n",
      "            ReLU-821            [-1, 256, 3, 3]               0\n",
      "          Conv2d-822           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-823           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-824           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-825           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-826            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-827            [-1, 256, 3, 3]             512\n",
      "            ReLU-828            [-1, 256, 3, 3]               0\n",
      "          Conv2d-829            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-830            [-1, 256, 3, 3]             512\n",
      "            ReLU-831            [-1, 256, 3, 3]               0\n",
      "          Conv2d-832           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-833           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-834           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-835           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-836            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-837            [-1, 256, 3, 3]             512\n",
      "            ReLU-838            [-1, 256, 3, 3]               0\n",
      "          Conv2d-839            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-840            [-1, 256, 3, 3]             512\n",
      "            ReLU-841            [-1, 256, 3, 3]               0\n",
      "          Conv2d-842           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-843           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-844           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-845           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-846            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-847            [-1, 256, 3, 3]             512\n",
      "            ReLU-848            [-1, 256, 3, 3]               0\n",
      "          Conv2d-849            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-850            [-1, 256, 3, 3]             512\n",
      "            ReLU-851            [-1, 256, 3, 3]               0\n",
      "          Conv2d-852           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-853           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-854           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-855           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-856            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-857            [-1, 256, 3, 3]             512\n",
      "            ReLU-858            [-1, 256, 3, 3]               0\n",
      "          Conv2d-859            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-860            [-1, 256, 3, 3]             512\n",
      "            ReLU-861            [-1, 256, 3, 3]               0\n",
      "          Conv2d-862           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-863           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-864           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-865           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-866            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-867            [-1, 256, 3, 3]             512\n",
      "            ReLU-868            [-1, 256, 3, 3]               0\n",
      "          Conv2d-869            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-870            [-1, 256, 3, 3]             512\n",
      "            ReLU-871            [-1, 256, 3, 3]               0\n",
      "          Conv2d-872           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-873           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-874           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-875           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-876            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-877            [-1, 256, 3, 3]             512\n",
      "            ReLU-878            [-1, 256, 3, 3]               0\n",
      "          Conv2d-879            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-880            [-1, 256, 3, 3]             512\n",
      "            ReLU-881            [-1, 256, 3, 3]               0\n",
      "          Conv2d-882           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-883           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-884           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-885           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-886            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-887            [-1, 256, 3, 3]             512\n",
      "            ReLU-888            [-1, 256, 3, 3]               0\n",
      "          Conv2d-889            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-890            [-1, 256, 3, 3]             512\n",
      "            ReLU-891            [-1, 256, 3, 3]               0\n",
      "          Conv2d-892           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-893           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-894           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-895           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-896            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-897            [-1, 256, 3, 3]             512\n",
      "            ReLU-898            [-1, 256, 3, 3]               0\n",
      "          Conv2d-899            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-900            [-1, 256, 3, 3]             512\n",
      "            ReLU-901            [-1, 256, 3, 3]               0\n",
      "          Conv2d-902           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-903           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-904           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-905           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-906            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-907            [-1, 256, 3, 3]             512\n",
      "            ReLU-908            [-1, 256, 3, 3]               0\n",
      "          Conv2d-909            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-910            [-1, 256, 3, 3]             512\n",
      "            ReLU-911            [-1, 256, 3, 3]               0\n",
      "          Conv2d-912           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-913           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-914           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-915           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-916            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-917            [-1, 256, 3, 3]             512\n",
      "            ReLU-918            [-1, 256, 3, 3]               0\n",
      "          Conv2d-919            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-920            [-1, 256, 3, 3]             512\n",
      "            ReLU-921            [-1, 256, 3, 3]               0\n",
      "          Conv2d-922           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-923           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-924           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-925           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-926            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-927            [-1, 256, 3, 3]             512\n",
      "            ReLU-928            [-1, 256, 3, 3]               0\n",
      "          Conv2d-929            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-930            [-1, 256, 3, 3]             512\n",
      "            ReLU-931            [-1, 256, 3, 3]               0\n",
      "          Conv2d-932           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-933           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-934           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-935           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-936            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-937            [-1, 256, 3, 3]             512\n",
      "            ReLU-938            [-1, 256, 3, 3]               0\n",
      "          Conv2d-939            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-940            [-1, 256, 3, 3]             512\n",
      "            ReLU-941            [-1, 256, 3, 3]               0\n",
      "          Conv2d-942           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-943           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-944           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-945           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-946            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-947            [-1, 256, 3, 3]             512\n",
      "            ReLU-948            [-1, 256, 3, 3]               0\n",
      "          Conv2d-949            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-950            [-1, 256, 3, 3]             512\n",
      "            ReLU-951            [-1, 256, 3, 3]               0\n",
      "          Conv2d-952           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-953           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-954           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-955           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-956            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-957            [-1, 256, 3, 3]             512\n",
      "            ReLU-958            [-1, 256, 3, 3]               0\n",
      "          Conv2d-959            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-960            [-1, 256, 3, 3]             512\n",
      "            ReLU-961            [-1, 256, 3, 3]               0\n",
      "          Conv2d-962           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-963           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-964           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-965           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-966            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-967            [-1, 256, 3, 3]             512\n",
      "            ReLU-968            [-1, 256, 3, 3]               0\n",
      "          Conv2d-969            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-970            [-1, 256, 3, 3]             512\n",
      "            ReLU-971            [-1, 256, 3, 3]               0\n",
      "          Conv2d-972           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-973           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-974           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-975           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-976            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-977            [-1, 256, 3, 3]             512\n",
      "            ReLU-978            [-1, 256, 3, 3]               0\n",
      "          Conv2d-979            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-980            [-1, 256, 3, 3]             512\n",
      "            ReLU-981            [-1, 256, 3, 3]               0\n",
      "          Conv2d-982           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-983           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-984           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-985           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-986            [-1, 256, 3, 3]         262,144\n",
      "     BatchNorm2d-987            [-1, 256, 3, 3]             512\n",
      "            ReLU-988            [-1, 256, 3, 3]               0\n",
      "          Conv2d-989            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-990            [-1, 256, 3, 3]             512\n",
      "            ReLU-991            [-1, 256, 3, 3]               0\n",
      "          Conv2d-992           [-1, 1024, 3, 3]         262,144\n",
      "     BatchNorm2d-993           [-1, 1024, 3, 3]           2,048\n",
      "            ReLU-994           [-1, 1024, 3, 3]               0\n",
      "      Bottleneck-995           [-1, 1024, 3, 3]               0\n",
      "          Conv2d-996            [-1, 512, 3, 3]         524,288\n",
      "     BatchNorm2d-997            [-1, 512, 3, 3]           1,024\n",
      "            ReLU-998            [-1, 512, 3, 3]               0\n",
      "          Conv2d-999            [-1, 512, 2, 2]       2,359,296\n",
      "    BatchNorm2d-1000            [-1, 512, 2, 2]           1,024\n",
      "           ReLU-1001            [-1, 512, 2, 2]               0\n",
      "         Conv2d-1002           [-1, 2048, 2, 2]       1,048,576\n",
      "    BatchNorm2d-1003           [-1, 2048, 2, 2]           4,096\n",
      "         Conv2d-1004           [-1, 2048, 2, 2]       2,097,152\n",
      "    BatchNorm2d-1005           [-1, 2048, 2, 2]           4,096\n",
      "           ReLU-1006           [-1, 2048, 2, 2]               0\n",
      "     Bottleneck-1007           [-1, 2048, 2, 2]               0\n",
      "         Conv2d-1008            [-1, 512, 2, 2]       1,048,576\n",
      "    BatchNorm2d-1009            [-1, 512, 2, 2]           1,024\n",
      "           ReLU-1010            [-1, 512, 2, 2]               0\n",
      "         Conv2d-1011            [-1, 512, 2, 2]       2,359,296\n",
      "    BatchNorm2d-1012            [-1, 512, 2, 2]           1,024\n",
      "           ReLU-1013            [-1, 512, 2, 2]               0\n",
      "         Conv2d-1014           [-1, 2048, 2, 2]       1,048,576\n",
      "    BatchNorm2d-1015           [-1, 2048, 2, 2]           4,096\n",
      "           ReLU-1016           [-1, 2048, 2, 2]               0\n",
      "     Bottleneck-1017           [-1, 2048, 2, 2]               0\n",
      "         Conv2d-1018            [-1, 512, 2, 2]       1,048,576\n",
      "    BatchNorm2d-1019            [-1, 512, 2, 2]           1,024\n",
      "           ReLU-1020            [-1, 512, 2, 2]               0\n",
      "         Conv2d-1021            [-1, 512, 2, 2]       2,359,296\n",
      "    BatchNorm2d-1022            [-1, 512, 2, 2]           1,024\n",
      "           ReLU-1023            [-1, 512, 2, 2]               0\n",
      "         Conv2d-1024           [-1, 2048, 2, 2]       1,048,576\n",
      "    BatchNorm2d-1025           [-1, 2048, 2, 2]           4,096\n",
      "           ReLU-1026           [-1, 2048, 2, 2]               0\n",
      "     Bottleneck-1027           [-1, 2048, 2, 2]               0\n",
      "AdaptiveAvgPool2d-1028           [-1, 2048, 1, 1]               0\n",
      "         Linear-1029                 [-1, 1000]       2,049,000\n",
      "         ResNet-1030                 [-1, 1000]               0\n",
      "================================================================\n",
      "Total params: 120,385,616\n",
      "Trainable params: 120,385,616\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 56.60\n",
      "Params size (MB): 459.23\n",
      "Estimated Total Size (MB): 515.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(resnet152).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 45,45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(resnet152.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (7.4955) | Acc_1: (0.00%) (0/64)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (3.0865) | Acc_1: (24.15%) (170/704)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (3.2912) | Acc_1: (23.96%) (322/1344)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (3.1396) | Acc_1: (24.27%) (471/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (340.7954) | Acc: (25.24%) (26/103)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (2.0358) | Acc_1: (31.25%) (20/64)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (1.7067) | Acc_1: (35.23%) (248/704)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (1.5713) | Acc_1: (39.73%) (534/1344)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (1.5140) | Acc_1: (41.58%) (807/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.3661) | Acc: (44.66%) (46/103)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (1.9307) | Acc_1: (45.31%) (29/64)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.5970) | Acc_1: (41.76%) (294/704)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.4892) | Acc_1: (43.68%) (587/1344)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.4199) | Acc_1: (46.42%) (901/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.2797) | Acc: (51.46%) (53/103)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.0341) | Acc_1: (53.12%) (34/64)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (1.1881) | Acc_1: (52.13%) (367/704)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (1.1692) | Acc_1: (53.05%) (713/1344)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (1.2669) | Acc_1: (51.73%) (1004/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1588) | Acc: (57.28%) (59/103)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (1.2708) | Acc_1: (56.25%) (36/64)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (1.3003) | Acc_1: (51.56%) (363/704)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (1.2164) | Acc_1: (52.53%) (706/1344)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (1.2674) | Acc_1: (51.00%) (990/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.7689) | Acc: (37.86%) (39/103)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (1.4410) | Acc_1: (45.31%) (29/64)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (1.3228) | Acc_1: (51.70%) (364/704)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (1.2360) | Acc_1: (50.82%) (683/1344)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (1.1776) | Acc_1: (52.76%) (1024/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3739) | Acc: (53.40%) (55/103)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (1.1507) | Acc_1: (54.69%) (35/64)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (1.2272) | Acc_1: (51.42%) (362/704)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (1.2759) | Acc_1: (52.01%) (699/1344)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (1.2240) | Acc_1: (53.01%) (1029/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.7590) | Acc: (48.54%) (50/103)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (1.4260) | Acc_1: (59.38%) (38/64)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (1.1337) | Acc_1: (54.55%) (384/704)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (1.1063) | Acc_1: (56.40%) (758/1344)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (1.1324) | Acc_1: (55.38%) (1075/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.2597) | Acc: (47.57%) (49/103)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (1.4605) | Acc_1: (45.31%) (29/64)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (1.1177) | Acc_1: (57.10%) (402/704)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (1.1378) | Acc_1: (57.89%) (778/1344)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (1.1389) | Acc_1: (57.86%) (1123/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1402) | Acc: (54.37%) (56/103)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (1.0851) | Acc_1: (56.25%) (36/64)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (1.0938) | Acc_1: (55.82%) (393/704)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (1.0383) | Acc_1: (56.70%) (762/1344)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (1.0417) | Acc_1: (57.50%) (1116/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.0909) | Acc: (45.63%) (47/103)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (1.2841) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (1.0489) | Acc_1: (60.80%) (428/704)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (1.0614) | Acc_1: (58.56%) (787/1344)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (1.0160) | Acc_1: (60.18%) (1168/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1573) | Acc: (50.49%) (52/103)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (0.6744) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.9660) | Acc_1: (60.23%) (424/704)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (1.0197) | Acc_1: (60.19%) (809/1344)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (0.9834) | Acc_1: (61.10%) (1186/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0472) | Acc: (64.08%) (66/103)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (0.8703) | Acc_1: (62.50%) (40/64)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (1.0475) | Acc_1: (61.08%) (430/704)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (1.0272) | Acc_1: (59.08%) (794/1344)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.9859) | Acc_1: (61.00%) (1184/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.7716) | Acc: (57.28%) (59/103)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.8699) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.9095) | Acc_1: (64.06%) (451/704)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.8653) | Acc_1: (66.82%) (898/1344)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.8485) | Acc_1: (65.64%) (1274/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.2156) | Acc: (54.37%) (56/103)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.8670) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.8488) | Acc_1: (64.49%) (454/704)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.8318) | Acc_1: (65.33%) (878/1344)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.8445) | Acc_1: (65.74%) (1276/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0353) | Acc: (59.22%) (61/103)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.7644) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.7463) | Acc_1: (69.89%) (492/704)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.7813) | Acc_1: (69.05%) (928/1344)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.8121) | Acc_1: (67.54%) (1311/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3101) | Acc: (59.22%) (61/103)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (1.2401) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.9211) | Acc_1: (66.90%) (471/704)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.7976) | Acc_1: (69.64%) (936/1344)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.8250) | Acc_1: (68.68%) (1333/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7760) | Acc: (70.87%) (73/103)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.8103) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.7392) | Acc_1: (72.16%) (508/704)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.7516) | Acc_1: (70.54%) (948/1344)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.7932) | Acc_1: (69.40%) (1347/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9173) | Acc: (57.28%) (59/103)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.8200) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.7046) | Acc_1: (70.60%) (497/704)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.7199) | Acc_1: (70.54%) (948/1344)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.7205) | Acc_1: (70.79%) (1374/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9622) | Acc: (63.11%) (65/103)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.7828) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.7419) | Acc_1: (68.18%) (480/704)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.7270) | Acc_1: (70.16%) (943/1344)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.7204) | Acc_1: (70.94%) (1377/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3509) | Acc: (62.14%) (64/103)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.9748) | Acc_1: (71.88%) (46/64)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.7078) | Acc_1: (73.86%) (520/704)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.6824) | Acc_1: (73.81%) (992/1344)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.6712) | Acc_1: (73.26%) (1422/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.7748) | Acc: (67.96%) (70/103)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (1.1580) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.6635) | Acc_1: (73.15%) (515/704)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.6566) | Acc_1: (72.92%) (980/1344)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.6567) | Acc_1: (73.36%) (1424/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7933) | Acc: (72.82%) (75/103)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.9240) | Acc_1: (57.81%) (37/64)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.6472) | Acc_1: (72.87%) (513/704)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.6421) | Acc_1: (73.29%) (985/1344)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.6484) | Acc_1: (73.93%) (1435/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9826) | Acc: (66.02%) (68/103)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.6673) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.6079) | Acc_1: (76.85%) (541/704)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.6000) | Acc_1: (76.71%) (1031/1344)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.6062) | Acc_1: (77.02%) (1495/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7634) | Acc: (67.96%) (70/103)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.5096) | Acc_1: (79.69%) (51/64)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.6058) | Acc_1: (76.42%) (538/704)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.6205) | Acc_1: (76.34%) (1026/1344)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.6293) | Acc_1: (75.89%) (1473/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7552) | Acc: (79.61%) (82/103)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.8052) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.5636) | Acc_1: (76.42%) (538/704)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.5688) | Acc_1: (77.01%) (1035/1344)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.5852) | Acc_1: (77.18%) (1498/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.7501) | Acc: (64.08%) (66/103)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.9172) | Acc_1: (60.94%) (39/64)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.5788) | Acc_1: (78.55%) (553/704)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.5666) | Acc_1: (77.08%) (1036/1344)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.5772) | Acc_1: (77.49%) (1504/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7631) | Acc: (72.82%) (75/103)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.5008) | Acc_1: (79.69%) (51/64)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.5425) | Acc_1: (77.98%) (549/704)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.5347) | Acc_1: (78.72%) (1058/1344)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.5527) | Acc_1: (78.05%) (1515/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4840) | Acc: (80.58%) (83/103)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.4405) | Acc_1: (79.69%) (51/64)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.4895) | Acc_1: (80.68%) (568/704)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.4799) | Acc_1: (81.62%) (1097/1344)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.5037) | Acc_1: (80.53%) (1563/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6446) | Acc: (75.73%) (78/103)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.5611) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.5467) | Acc_1: (78.69%) (554/704)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.5288) | Acc_1: (79.84%) (1073/1344)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.5123) | Acc_1: (80.06%) (1554/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6870) | Acc: (70.87%) (73/103)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.3734) | Acc_1: (87.50%) (56/64)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.4654) | Acc_1: (81.11%) (571/704)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.4857) | Acc_1: (80.13%) (1077/1344)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.4853) | Acc_1: (81.04%) (1573/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5621) | Acc: (74.76%) (77/103)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.2896) | Acc_1: (90.62%) (58/64)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.4602) | Acc_1: (80.54%) (567/704)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.4592) | Acc_1: (81.47%) (1095/1344)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.4843) | Acc_1: (80.37%) (1560/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5187) | Acc: (76.70%) (79/103)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.3996) | Acc_1: (84.38%) (54/64)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.5064) | Acc_1: (79.40%) (559/704)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.4745) | Acc_1: (80.80%) (1086/1344)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.4858) | Acc_1: (81.04%) (1573/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6910) | Acc: (71.84%) (74/103)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.6699) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.5426) | Acc_1: (80.26%) (565/704)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.4927) | Acc_1: (81.55%) (1096/1344)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.4912) | Acc_1: (81.40%) (1580/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4553) | Acc: (83.50%) (86/103)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.3276) | Acc_1: (89.06%) (57/64)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.3956) | Acc_1: (83.95%) (591/704)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.4224) | Acc_1: (82.66%) (1111/1344)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.4444) | Acc_1: (83.20%) (1615/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6264) | Acc: (80.58%) (83/103)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.3630) | Acc_1: (89.06%) (57/64)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.4314) | Acc_1: (83.81%) (590/704)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.4268) | Acc_1: (83.48%) (1122/1344)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.4463) | Acc_1: (82.74%) (1606/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6736) | Acc: (77.67%) (80/103)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.4539) | Acc_1: (84.38%) (54/64)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.3524) | Acc_1: (87.78%) (618/704)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.3576) | Acc_1: (86.46%) (1162/1344)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.3863) | Acc_1: (85.83%) (1666/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7010) | Acc: (74.76%) (77/103)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.3646) | Acc_1: (85.94%) (55/64)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.4117) | Acc_1: (85.80%) (604/704)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.4331) | Acc_1: (83.18%) (1118/1344)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.4407) | Acc_1: (83.36%) (1618/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5811) | Acc: (80.58%) (83/103)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.5746) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.4395) | Acc_1: (81.39%) (573/704)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.4283) | Acc_1: (82.66%) (1111/1344)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.4455) | Acc_1: (82.48%) (1601/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3197) | Acc: (85.44%) (88/103)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.3469) | Acc_1: (84.38%) (54/64)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.4840) | Acc_1: (82.24%) (579/704)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.4191) | Acc_1: (83.56%) (1123/1344)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.4199) | Acc_1: (84.39%) (1638/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0770) | Acc: (74.76%) (77/103)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.5074) | Acc_1: (78.12%) (50/64)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.3783) | Acc_1: (85.37%) (601/704)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.3591) | Acc_1: (85.57%) (1150/1344)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.3729) | Acc_1: (85.37%) (1657/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5312) | Acc: (80.58%) (83/103)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.3626) | Acc_1: (84.38%) (54/64)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.3615) | Acc_1: (85.80%) (604/704)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.3810) | Acc_1: (85.57%) (1150/1344)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.4064) | Acc_1: (85.52%) (1660/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6222) | Acc: (75.73%) (78/103)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.5155) | Acc_1: (79.69%) (51/64)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.3998) | Acc_1: (84.94%) (598/704)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.3702) | Acc_1: (86.31%) (1160/1344)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.3712) | Acc_1: (86.14%) (1672/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7453) | Acc: (72.82%) (75/103)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (1.1318) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.4637) | Acc_1: (83.66%) (589/704)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.4118) | Acc_1: (85.12%) (1144/1344)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.4030) | Acc_1: (85.11%) (1652/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4859) | Acc: (80.58%) (83/103)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.2213) | Acc_1: (90.62%) (58/64)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.3119) | Acc_1: (86.22%) (607/704)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.3227) | Acc_1: (87.20%) (1172/1344)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.3270) | Acc_1: (87.07%) (1690/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4467) | Acc: (79.61%) (82/103)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.4644) | Acc_1: (82.81%) (53/64)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.3398) | Acc_1: (87.78%) (618/704)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.3321) | Acc_1: (87.50%) (1176/1344)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.3239) | Acc_1: (88.25%) (1713/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6339) | Acc: (77.67%) (80/103)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.3086) | Acc_1: (89.06%) (57/64)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.3116) | Acc_1: (88.35%) (622/704)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.3201) | Acc_1: (88.24%) (1186/1344)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.3307) | Acc_1: (88.15%) (1711/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8624) | Acc: (68.93%) (71/103)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.7339) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.4440) | Acc_1: (85.09%) (599/704)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.3866) | Acc_1: (86.53%) (1163/1344)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.3826) | Acc_1: (86.81%) (1685/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.1150) | Acc: (75.73%) (78/103)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.4830) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.3425) | Acc_1: (85.51%) (602/704)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.3119) | Acc_1: (86.98%) (1169/1344)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.3583) | Acc_1: (86.91%) (1687/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5613) | Acc: (78.64%) (81/103)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.2532) | Acc_1: (90.62%) (58/64)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.3394) | Acc_1: (86.79%) (611/704)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.3085) | Acc_1: (88.39%) (1188/1344)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.3340) | Acc_1: (87.84%) (1705/1941)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5408) | Acc: (82.52%) (85/103)\n",
      "2 hours 22 mins 58 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet152.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 50):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='resnet152.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_img(img_path, model):\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # opencv BGR read.\n",
    "    img = cv2.resize(img, (45, 45))\n",
    "    img = torch.from_numpy(img).float()\n",
    "\n",
    "    img = img.permute(2, 0, 1).squeeze(0) # (H, W, C) -> (C, H, W) -> (1, C, H, W)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.Softmax()\n",
    "\n",
    "    out = model(img)\n",
    "    label_idx = torch.argmax(out, dim=1)\n",
    "    prob = criterion(out)\n",
    "\n",
    "    return prob, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/hom1/ict01/.conda/envs/tomato_cls/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 hours 0 mins 10 secs for training\n"
     ]
    }
   ],
   "source": [
    "default_directory = './save_models'\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet152.tar.gz')\n",
    "\n",
    "# model   MyNetwork .\n",
    "\n",
    "model = models.resnet152()\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "#model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for img in glob.iglob('./data/test/**/*.jpg', recursive=True):\n",
    "    pred, label_idx = classification_img(img, model)\n",
    "\n",
    "now = time.gmtime(time.time() - start)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea228a1e57bab7175f6663bb7b03ad0c48b776b0d8531c957bf1be729535464c"
  },
  "kernelspec": {
   "display_name": "tomato_cls",
   "language": "python",
   "name": "tomato_cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
