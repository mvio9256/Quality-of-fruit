{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1,2,3,4,5'\n",
    "start_time = time.time()\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/cus_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = datasets.ImageFolder(\"./data/train\",\n",
    "                         transform=transforms.Compose([transforms.Resize(64),\n",
    "                                                       transforms.RandomCrop(45),\n",
    "                                                       transforms.ToTensor()]))\n",
    "\n",
    "test_imgs = datasets.ImageFolder(\"./data/test\",\n",
    "                        transform=transforms.Compose([transforms.Resize(64),\n",
    "                                                      transforms.RandomCrop(45),\n",
    "                                                      transforms.ToTensor()]))\n",
    "\n",
    "train_loader = DataLoader(train_imgs, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_imgs, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer1_conv = nn.Conv2d(3, 64, 3, 1) # 입력 1개, 출력 6개, 필터 크기는 5x5, 1칸 단위로 이동하면서 필터를 씌운다\n",
    "        self.layer1_relu = nn.ReLU()             # 활성화 함수. ReLU(x) 는 max(x, 0)과 같다\n",
    "        self.layer1_pool = nn.MaxPool2d(2)       # 각 2x2 칸마다 최대값 하나씩만을 남긴다\n",
    "        self.layer2_conv = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.layer2_relu = nn.ReLU()\n",
    "        self.layer2_pool = nn.MaxPool2d(2)\n",
    "        self.layer3_conv = nn.Conv2d(64, 32, 3, 1)\n",
    "        self.layer3_relu = nn.ReLU()\n",
    "        self.layer3_pool = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(32*3*3, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1_conv(x)  # 1x28x28 형식의 데이터가 6x24x24 형식으로 변환된다\n",
    "        x2 = self.layer1_relu(x1) # \n",
    "        x3 = self.layer1_pool(x2) # 6x24x24 형식의 데이터가 6x12x12 형식으로 변환된다\n",
    "        x4 = self.layer2_conv(x3) # 6x12x12 형식의 데이터가 16x8x8 형식으로 변환된다\n",
    "        x5 = self.layer2_relu(x4) # \n",
    "        x6 = self.layer2_pool(x5) # 16x8x8 형식의 데이터가 16x4x4 형식으로 변환된다\n",
    "        x7 = self.layer3_conv(x6) # 6x12x12 형식의 데이터가 16x8x8 형식으로 변환된다\n",
    "        x8 = self.layer3_relu(x7) # \n",
    "        x9 = self.layer3_pool(x8)\n",
    "        #print(x9.size())\n",
    "        x10 = x9.view(-1, 288)     # 16x4x4 형식의 데이터가 256-벡터로 변환된다\n",
    "        x11 = self.fc(x10)          # 256-벡터가 10-벡터로 변환된다\n",
    "        return x11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_cnn = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 43, 43]           1,792\n",
      "              ReLU-2           [-1, 64, 43, 43]               0\n",
      "         MaxPool2d-3           [-1, 64, 21, 21]               0\n",
      "            Conv2d-4           [-1, 64, 19, 19]          36,928\n",
      "              ReLU-5           [-1, 64, 19, 19]               0\n",
      "         MaxPool2d-6             [-1, 64, 9, 9]               0\n",
      "            Conv2d-7             [-1, 32, 7, 7]          18,464\n",
      "              ReLU-8             [-1, 32, 7, 7]               0\n",
      "         MaxPool2d-9             [-1, 32, 3, 3]               0\n",
      "           Linear-10                    [-1, 4]           1,156\n",
      "================================================================\n",
      "Total params: 58,340\n",
      "Trainable params: 58,340\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.44\n",
      "Params size (MB): 0.22\n",
      "Estimated Total Size (MB): 2.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#if torch.cuda.device_count() > 0:\n",
    "#    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#    model = nn.DataParallel(cus_cnn).cuda()\n",
    "#    cudnn.benchmark = True\n",
    "#else:\n",
    "#    print(\"USE ONLY CPU!\")\n",
    "model = cus_cnn.cuda()\n",
    "\n",
    "summary(model,(3, 45,45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(cus_cnn.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "        print(data.shape)\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (1.3823) | Acc_1: (29.69%) (19/64)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (1.3842) | Acc_1: (29.12%) (205/704)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (1.3824) | Acc_1: (30.43%) (409/1344)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (1.3801) | Acc_1: (30.45%) (591/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (1.3780) | Acc: (30.10%) (31/103)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (1.3792) | Acc_1: (23.44%) (15/64)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (1.3721) | Acc_1: (28.41%) (200/704)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (1.3685) | Acc_1: (28.42%) (382/1344)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (1.3615) | Acc_1: (30.09%) (584/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (1.3508) | Acc: (31.07%) (32/103)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (1.2965) | Acc_1: (42.19%) (27/64)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.3351) | Acc_1: (33.66%) (237/704)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.3172) | Acc_1: (36.61%) (492/1344)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.2955) | Acc_1: (38.49%) (747/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (1.1953) | Acc: (48.54%) (50/103)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.2132) | Acc_1: (50.00%) (32/64)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (1.2025) | Acc_1: (45.31%) (319/704)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (1.1497) | Acc_1: (48.59%) (653/1344)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (1.1264) | Acc_1: (49.41%) (959/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (1.0359) | Acc: (54.37%) (56/103)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (1.0857) | Acc_1: (54.69%) (35/64)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (1.0536) | Acc_1: (53.69%) (378/704)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (1.0404) | Acc_1: (52.90%) (711/1344)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (1.0307) | Acc_1: (53.32%) (1035/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (1.0366) | Acc: (49.51%) (51/103)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (1.1004) | Acc_1: (39.06%) (25/64)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (1.0092) | Acc_1: (53.84%) (379/704)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (0.9948) | Acc_1: (53.42%) (718/1344)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (0.9892) | Acc_1: (53.94%) (1047/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (1.1012) | Acc: (54.37%) (56/103)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (0.8303) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (0.9446) | Acc_1: (55.40%) (390/704)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (0.9543) | Acc_1: (56.77%) (763/1344)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (0.9560) | Acc_1: (57.03%) (1107/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (1.2390) | Acc: (51.46%) (53/103)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (1.1868) | Acc_1: (50.00%) (32/64)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (0.9289) | Acc_1: (58.10%) (409/704)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (0.9262) | Acc_1: (58.41%) (785/1344)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (0.9370) | Acc_1: (57.29%) (1112/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.9137) | Acc: (54.37%) (56/103)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (0.7376) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (0.8854) | Acc_1: (59.66%) (420/704)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (0.8964) | Acc_1: (59.38%) (798/1344)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (0.8988) | Acc_1: (59.45%) (1154/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.9054) | Acc: (54.37%) (56/103)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (0.8399) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (0.8744) | Acc_1: (60.94%) (429/704)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (0.8986) | Acc_1: (60.79%) (817/1344)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (0.8883) | Acc_1: (61.77%) (1199/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8962) | Acc: (51.46%) (53/103)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (0.7991) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (0.8498) | Acc_1: (62.07%) (437/704)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (0.8821) | Acc_1: (61.09%) (821/1344)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (0.8690) | Acc_1: (61.31%) (1190/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.9060) | Acc: (53.40%) (55/103)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (0.8270) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.8395) | Acc_1: (61.36%) (432/704)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (0.8838) | Acc_1: (60.86%) (818/1344)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (0.8823) | Acc_1: (61.26%) (1189/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.9826) | Acc: (51.46%) (53/103)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (0.9516) | Acc_1: (54.69%) (35/64)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.8402) | Acc_1: (61.36%) (432/704)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.8677) | Acc_1: (59.82%) (804/1344)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.8466) | Acc_1: (61.15%) (1187/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8960) | Acc: (55.34%) (57/103)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.9589) | Acc_1: (57.81%) (37/64)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.8610) | Acc_1: (59.66%) (420/704)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.8550) | Acc_1: (60.04%) (807/1344)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.8337) | Acc_1: (61.88%) (1201/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8333) | Acc: (61.17%) (63/103)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.6894) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.8071) | Acc_1: (63.35%) (446/704)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.7932) | Acc_1: (64.66%) (869/1344)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.8428) | Acc_1: (63.37%) (1230/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8519) | Acc: (57.28%) (59/103)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.8924) | Acc_1: (57.81%) (37/64)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.8071) | Acc_1: (63.35%) (446/704)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.7979) | Acc_1: (63.39%) (852/1344)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.7979) | Acc_1: (64.25%) (1247/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8542) | Acc: (55.34%) (57/103)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.7516) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.8070) | Acc_1: (64.35%) (453/704)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.7863) | Acc_1: (64.81%) (871/1344)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.7786) | Acc_1: (64.14%) (1245/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8026) | Acc: (61.17%) (63/103)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.7027) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.7462) | Acc_1: (66.62%) (469/704)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.7592) | Acc_1: (66.74%) (897/1344)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.7533) | Acc_1: (66.87%) (1298/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8712) | Acc: (62.14%) (64/103)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.6279) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.7509) | Acc_1: (68.04%) (479/704)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.7476) | Acc_1: (67.78%) (911/1344)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.7469) | Acc_1: (67.18%) (1304/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7642) | Acc: (70.87%) (73/103)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.6019) | Acc_1: (75.00%) (48/64)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.7009) | Acc_1: (67.90%) (478/704)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.7241) | Acc_1: (67.86%) (912/1344)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.7408) | Acc_1: (66.82%) (1297/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7870) | Acc: (63.11%) (65/103)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.6751) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.7658) | Acc_1: (68.18%) (480/704)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.7366) | Acc_1: (67.93%) (913/1344)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.7380) | Acc_1: (66.72%) (1295/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8054) | Acc: (63.11%) (65/103)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.7633) | Acc_1: (64.06%) (41/64)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.7432) | Acc_1: (67.61%) (476/704)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.7011) | Acc_1: (69.64%) (936/1344)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.7153) | Acc_1: (69.04%) (1340/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7673) | Acc: (61.17%) (63/103)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.7384) | Acc_1: (60.94%) (39/64)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.6924) | Acc_1: (68.47%) (482/704)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.7466) | Acc_1: (67.26%) (904/1344)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.7331) | Acc_1: (67.59%) (1312/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7790) | Acc: (70.87%) (73/103)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.7190) | Acc_1: (68.75%) (44/64)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.6863) | Acc_1: (70.17%) (494/704)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.6929) | Acc_1: (70.24%) (944/1344)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.6945) | Acc_1: (70.58%) (1370/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8650) | Acc: (61.17%) (63/103)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.6974) | Acc_1: (67.19%) (43/64)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.7028) | Acc_1: (69.18%) (487/704)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.6653) | Acc_1: (70.46%) (947/1344)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.6815) | Acc_1: (70.22%) (1363/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8134) | Acc: (64.08%) (66/103)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.6483) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.6710) | Acc_1: (71.45%) (503/704)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.6709) | Acc_1: (72.25%) (971/1344)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.6675) | Acc_1: (72.23%) (1402/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.6764) | Acc: (71.84%) (74/103)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.5136) | Acc_1: (79.69%) (51/64)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.6731) | Acc_1: (70.74%) (498/704)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.6680) | Acc_1: (70.68%) (950/1344)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.6620) | Acc_1: (70.69%) (1372/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.6758) | Acc: (69.90%) (72/103)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.5490) | Acc_1: (81.25%) (52/64)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.5994) | Acc_1: (74.29%) (523/704)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.6365) | Acc_1: (72.77%) (978/1344)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.6399) | Acc_1: (72.85%) (1414/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7053) | Acc: (67.96%) (70/103)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.7368) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.6731) | Acc_1: (70.03%) (493/704)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.6469) | Acc_1: (73.51%) (988/1344)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.6484) | Acc_1: (73.72%) (1431/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7270) | Acc: (64.08%) (66/103)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.6812) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.6426) | Acc_1: (72.87%) (513/704)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.6273) | Acc_1: (73.51%) (988/1344)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.6379) | Acc_1: (72.80%) (1413/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7891) | Acc: (68.93%) (71/103)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.6015) | Acc_1: (73.44%) (47/64)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.6392) | Acc_1: (71.31%) (502/704)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.6493) | Acc_1: (70.76%) (951/1344)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.6622) | Acc_1: (71.46%) (1387/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8470) | Acc: (63.11%) (65/103)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.6996) | Acc_1: (59.38%) (38/64)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.6352) | Acc_1: (72.73%) (512/704)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.6247) | Acc_1: (72.77%) (978/1344)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.6393) | Acc_1: (72.18%) (1401/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.6730) | Acc: (71.84%) (74/103)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.7260) | Acc_1: (65.62%) (42/64)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.5640) | Acc_1: (75.43%) (531/704)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.6010) | Acc_1: (75.52%) (1015/1344)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.5924) | Acc_1: (75.43%) (1464/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7201) | Acc: (65.05%) (67/103)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.5665) | Acc_1: (79.69%) (51/64)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.6071) | Acc_1: (74.72%) (526/704)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.6208) | Acc_1: (74.48%) (1001/1344)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.6152) | Acc_1: (75.01%) (1456/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.6841) | Acc: (71.84%) (74/103)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.5828) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.6329) | Acc_1: (71.45%) (503/704)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.6205) | Acc_1: (72.32%) (972/1344)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.6178) | Acc_1: (72.90%) (1415/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.9618) | Acc: (58.25%) (60/103)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.6413) | Acc_1: (71.88%) (46/64)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.5719) | Acc_1: (76.70%) (540/704)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.5969) | Acc_1: (75.15%) (1010/1344)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.6023) | Acc_1: (74.34%) (1443/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7796) | Acc: (66.02%) (68/103)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.6313) | Acc_1: (70.31%) (45/64)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.6048) | Acc_1: (73.01%) (514/704)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.6087) | Acc_1: (73.44%) (987/1344)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.6095) | Acc_1: (73.47%) (1426/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.7009) | Acc: (76.70%) (79/103)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.5241) | Acc_1: (81.25%) (52/64)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.5921) | Acc_1: (74.15%) (522/704)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.5686) | Acc_1: (75.60%) (1016/1344)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.5642) | Acc_1: (75.79%) (1471/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.6818) | Acc: (74.76%) (77/103)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.5586) | Acc_1: (76.56%) (49/64)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.6432) | Acc_1: (72.73%) (512/704)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.5899) | Acc_1: (75.67%) (1017/1344)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.5666) | Acc_1: (77.18%) (1498/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.6423) | Acc: (76.70%) (79/103)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.4403) | Acc_1: (84.38%) (54/64)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.5439) | Acc_1: (77.27%) (544/704)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.5177) | Acc_1: (77.38%) (1040/1344)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.5328) | Acc_1: (76.71%) (1489/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.8134) | Acc: (63.11%) (65/103)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.8196) | Acc_1: (59.38%) (38/64)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.5573) | Acc_1: (77.13%) (543/704)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.5456) | Acc_1: (77.68%) (1044/1344)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.5448) | Acc_1: (77.90%) (1512/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.6482) | Acc: (68.93%) (71/103)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.5041) | Acc_1: (81.25%) (52/64)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.5481) | Acc_1: (78.55%) (553/704)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.5393) | Acc_1: (78.20%) (1051/1344)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.5277) | Acc_1: (78.21%) (1518/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.5785) | Acc: (73.79%) (76/103)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.4478) | Acc_1: (82.81%) (53/64)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.5261) | Acc_1: (78.12%) (550/704)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.5136) | Acc_1: (79.32%) (1066/1344)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.5177) | Acc_1: (79.24%) (1538/1941)\n",
      "=> saving checkpoint\n",
      "torch.Size([64, 3, 45, 45])\n",
      "torch.Size([39, 3, 45, 45])\n",
      "# TEST : Loss: (0.6613) | Acc: (68.93%) (71/103)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.8537) | Acc_1: (56.25%) (36/64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5019/962655141.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     save_checkpoint(default_directory, {\n",
      "\u001b[0;32m/tmp/ipykernel_5019/2360500545.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0miters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tomato_cls/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tomato_cls/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tomato_cls/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tomato_cls/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tomato_cls/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[1;32m    231\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tomato_cls/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tomato_cls/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tomato_cls/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transparency\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tomato_cls/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='cus_cnn.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 50):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='cus_cnn.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea228a1e57bab7175f6663bb7b03ad0c48b776b0d8531c957bf1be729535464c"
  },
  "kernelspec": {
   "display_name": "tomato_cls",
   "language": "python",
   "name": "tomato_cls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
